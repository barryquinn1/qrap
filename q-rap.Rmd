---
title: "Infrastructure and toolkit for teaching financial machine learning"
description: |
 A case study on embedding computation as a central tennent of a finance curicullum.
author:
  - name: Barry Quinn 
    url: https://quinference.com
    affiliation: Queens Management School
    affiliation_url: https://qub.ac.uk/mgt
date: "`r Sys.Date()`"
bibliography: biblio.bib
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# Introduction

Many parts of modern finance are fundamentally quantitative, with financial researcher increasing solving problems using innovative financial technology solutions.  Furthermore, the rise of big and alternative data in combination with the exponential growth of AI and financial data science also is created new opportunities in financial sector, including risk management, portfolio construction, investment banking and insurance.  Given the finance professionals have an important fiduciary duty towards their clients, the rapid growth on AI in finance has highlighted some important risks around trust, overfitting, lack of interpretability, biased inputs and unethical use of data.  Now more than ever highly computationally literate finance graduates are needed to balance the exponential growth of artificial intelligence(AI)/data science with ethics, bias, and privacy to create *trustworthy* data-driven decisions [@Mahdavi2020].

Finance and computation has gone hand in hand for centuries, with quantitative finance taking its roots from Bachelier’s *Theory of Speculation* (Bachelier 1900). On the buy-side: portfolio management was transformed with the quantitative research of Harry Markowitz in the early 1950’s shows how a complex mean-variance portfolio optimisation problem could be approximated by his Critical Line algorithm. Computerised algorithm trading championed by Ed Thorp, John Simons in the 1960’s shows arbitrage opportunities, unseen by traditional hedge fund managers, could be exploited to consistently *beat the market*.  On the sell-side, a game changing breakthrough in the 1970s was an options pricing model [@Black1973;@Merton1973], targeting explosive growth in the options markets [@Cesa2017]. Ironically, the BSM model main weaknesses, seen a growth in financial computing demands, as quantitative researchers use more realistic continuous-time pricing models which require computationally challenging partial differential equations [@Reisinger2018].  Figure 1 summarise some landmark moments in cloud computing and quantitative finance.


```{r timeline}
library(vistime)
library(ggrepel)
library(ggdark)
library(stringr)

c("Harry Markowitz <br> introduces <br> Critical Line Algorithm",
  "NASDAQ launched <br> as first electronic <br> communications market",
  "Fischer Black <br> proposes idea of fully <br> electronic exchanges <br> in a landmark paper",
  "Black-Scholes-Merton <br> model for derivative pricing",
  "Jim Simons <br> Founded Renaissance Technologies (RenTec), <br> introducing complex mathematical trading algorithms to markets",
  "Michael Bloomberg <br> launches Innovative Market Systems <br> (which become Bloomberg LLP)",
  "RenTec's Medallion fund <br> launched, <br> later to become <br> the most successful hedge fund in history",
  "Heston & Dupire <br> introduce stochastic volatility models",
  "Jump diffusion <br> models introduced",
  "SEC<br>order US stock<br>exchanges to be decimalised",
  "Flash Crash <br> (Markets drop 10% in a matter of minutes)",
  "Basel III <br> requires to periodical estimate <br> counterparty risk of complex derivatives",
  "Quantum computing <br> is proposed by Renbart et. al <br> for derivative pricing",
  "RenTex's Medallion <br> Fund average 66% annual return <br> over last 30 years",
  "CME Smart Stream <br> launched offering real-time <br> cloud-based market data")->financeMs
  

c("Professor JohnMcCarthy (MIT) <br> suggested computing will be sold as a utility",
  "IBM <br> virtualised operating systems",
  "ARPANET <br> launched by US Advanced <br> Research Project Agency <br> connect 4 university computer systems",
  "100,000 <br> computers on Internet",
  "World Wide Web <br> lanuched with 1 million computers on Net",
  "Cloud Computing <br> as a concept <br> introduced in Compaq report",
  "Amazon (AWS) <br> launched as first public cloud service",
  "Big Data <br> as a concept <br> introduced by the <br>OpenNebula research project launched",
  "Elastic Computing (EC2) <br> launched by Amazon",
  "Dropbox <br> launch cloud storage",
  "Microsoft <br> launch Azure cloud computing",
  "DigitalOcean <br> Droplets launched",
  "Real-time <br> streaming data on AWS",
  "Machine learning <br> sold as a service in the Cloud",
  "Massive data-center <br> under the Altantic ocean <br> launched by Microsoft",
  "Google TPU's <br> (Tensor Processing Units) <br> avaliable on the cloud, <br> introducing tensor-based mathematical to public")->cloudMs            
content<-c(financeMs,cloudMs)            
start   = paste0(c("1952","1971","1972","1973","1982","1983","1988","1993","2000","2001","2010","2013","2017","2018","2019",
                   "1961","1967","1969","1988","1991","1996","2002","2005","2006","2007","2010","2012","2013","2015","2018","2019"),"-01-01")

i<-length(financeMs)
j=length(cloudMs)

timeline_data<-data.frame(
  event =str_wrap(content,width=5),
  start=start,
  end=start,
  group=c(rep("Finance",i),rep("Cloud <br> computing",j)),
  fontcolor=c(rep("",i),rep("blue",j)))

p<-hc_vistime(timeline_data,col.color = "fontcolor",title = "Figure 1: Timeline of landscape computing events in finance and cloud computing")
library(highcharter)

thm <- hc_theme(
  colors = c("red", "green", "blue"),
  chart = list(
    backgroundColor = "lightgray"),
  title = list(
    style = list(
      color = "#333333",
      fontFamily = "Shadows Into Light",
      fontSize=25)),
  subtitle = list(
    style = list(
      color = "#666666",
      fontFamily = "Shadows Into Light")),
  legend = list(
    itemStyle = list(
      fontFamily = "Tangerine",
      color = "black"),
    itemHoverStyle = list(
      color = "gray"
    )
  )
)

p %>%
  hc_add_theme(thm)
```

<figurecaption> [@Varghese2019] provides the source of the cloud computing timeline, while the timeline for computational finance events is the authors own calculations. <figurecaption/>

## What is financial machine learning?
Machine learning has been adopted at pace in many real world applications, but has been slow to develop in areas of scientific research, especially financial research where tradition econometric techniques dominate.  It has been argued that this is due to a clashing culture, where the financial economics academia argue the ontological differences in econometrics and machine learning are intractable [@Coulombe2019]. This naive comparison highlights the epistemological challenges that are faced by computer age statistical inference of the rapid expansion of algorithmic development[@Efron2016]. Financial machine learning is a sub field in its infancy in the academic literature, which is attempting to reconcile the large differences between econometrics and machine learning by

Machine learning can be defined as a branch of nonparametric statistics mixing, statistical learning, computer science and optimisation [@Molin2019] where algorithms are made up of three fundamental building blocks

1. A loss function
2. An optimisation criteria
3. A optimisation routine    

Changes in each of these building blocks produces a wide variation of learning algorithms characterising the degree of supervision. It is important to remember that these models are biased due to their optimisation of a restricted objective according to a specific algorithmic methodology and statistical rationale.  In contrast, Econometrics applies statistics, usually in the form of a regression analysis, to examine relationships. Model designs using a prior hypothesised model based on well journeyed theories, to produce objective (by minimising bias) statistical inference.  

Broadly speaking financial machine learning attempts to resolve three broad conflicts between machine learning and financial economic research[@Lommers2021]:

1. The importance of statistical inference
2. Causality
3. A prior hypotheses and model assumptions

### Statistical inference
Statistical inference broad discipline at the intersection of mathematics, empirical science and philoshopy. From its philosophical beginings through the publication of the Bayes rule in 1763 (used by early advocates to argue the existence of god) to its most recent advances in computation, a traditional bottleneck to statistical applications up to the early 1950s[@Efron2016].

Statistical inference is the bedrock of econometrics, while the main focus of machine learning is prediction. In traditional econometrics, models are build to learn statistical information and uncertainty about the parameters of the underlying data generating process, using an a prior probability model of the data generating process, with a proven theoretical track record under certain assumptions. In contrasts, machine learning models the focus is on prediction output, where the data generated process is generally undefined, with the goal of algorithmically optimisation models to fit the underlying data generating process as well as possible [@Lommers2021]. [@Efron2016] summaries this well in their definition of computer age statistical inference 

<blockquote>
 Very broadly speaking, algorithms are what statistician do while inference says why they do them…The efflorescence of ambitious algorithms has forced an evolution (though not a revolution) in inference, the theories by which statisticians choose among competing methods. 
</blockquote>


# Computing environment
To understand more about the computational foundations for students we borrow from the extant statistical education literature [@Kaplan2007; @Cetinkaya-Rundel2018].  Much like teaching statistics/data science quantitative finance has two interconnected goals. 
1. Get students to do something interesting with data (and code) within the first ten minutes of the first class.
2. Get students to think about computation as an integral part of the quantitative finance curriculum.
An common solution is to use computing labs to facilitate computation exercises.  The downside is that instructors usually do not have administrative access and therefore struggle accomplish even the basic maintenance tasks.  Furthermore, this usually leads to common environment for all courses, rather than specialised set-ups for more enhanced student computational needs.  Finally, to achieve the second goal requires active engagement of computation for all contact time. 
We use a web browser-based solutions, RStudio’s Workbench, to provide a frictionless student experience in both lectures and lab sessions. Workbench is professional data science server software, where facilitates interoperable computational insight integrate R and Python leveraging support for Jupyter, VSCode, and RStudio’s integrated development enviroment (IDE) [@RStudio2021].

## Why R and Python?

## Why RStudio?

## Solving the **but it worked on my machine?** problem

# Reference

Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.

