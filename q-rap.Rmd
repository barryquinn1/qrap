---
title: "Infrastructure and toolkit for teaching financial machine learning"
description: |
 A case study on embedding computation as a central tennent of a finance curicullum.
author:
  - name: Barry Quinn 
    url: https://quinference.com
    affiliation: Queens Management School
    affiliation_url: https://qub.ac.uk/mgt
date: "`r Sys.Date()`"
bibliography: biblio.bib
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# Introduction

While finance is a social science, many parts of modern finance are fundamentally quantitative, with financial researcher increasing solving problems using innovative technological solutions.  Furthermore, the rise of big and alternative data in combination with the exponential growth of AI and financial data science is created new opportunities in financial sector, including risk management, portfolio construction, investment banking and insurance.  Given that finance professionals have an important fiduciary duty towards their clients, the rapid growth on AI in finance has highlighted some important risks around trust, overfitting, lack of interpretability, biased inputs and unethical use of data.  Now more than ever highly computationally literate finance graduates are needed to balance the exponential growth of artificial intelligence(AI)/data science with ethics, bias, and privacy to create *trustworthy* data-driven decisions [@Mahdavi2020].

Finance and computation has gone hand in hand for centuries, with quantitative finance taking its roots from Bachelier’s *Theory of Speculation* (Bachelier 1900). On the buy-side: portfolio management was transformed with the quantitative research of Harry Markowitz in the early 1950’s. Markowitz showed how a complex mean-variance portfolio optimisation problem could be approximated using algorithmic programming. Meanwhile, in the early 1960's [Ed Thorp](https://en.wikipedia.org/wiki/Edward_O._Thorp) and [John Simons](https://en.wikipedia.org/wiki/Jim_Simons_(mathematician)) using computer-aided statistical algorithms to show how arbitrage opportunities, unseen by traditional hedge fund managers, could be exploited to consistently *beat the market*.  On the sell-side, a game changing breakthrough in the 1970s was a model to price derivative products [@Black1973;@Merton1973] (BSM model), resulting in explosive growth in the options markets [@Cesa2017]. Subsequently weaknesses in the BSM model fuelled a growth in financial computing. Quantitative researchers, with the increased available of computational power, used more realistic continuous-time pricing models to estimate complex partial differential equations [@Reisinger2018]. This progression on computing and quantitative finance is nicely summarise in Figure 1.


```{r timeline}
library(vistime)
library(ggrepel)
library(ggdark)
library(stringr)

c("Harry Markowitz <br> introduces <br> Critical Line Algorithm",
  "NASDAQ launched <br> as first electronic <br> communications market",
  "Fischer Black <br> proposes idea of fully <br> electronic exchanges <br> in a landmark paper",
  "Black-Scholes-Merton <br> model for derivative pricing",
  "Jim Simons <br> Founded Renaissance Technologies (RenTec), <br> introducing complex mathematical trading algorithms to markets",
  "Michael Bloomberg <br> launches Innovative Market Systems <br> (which become Bloomberg LLP)",
  "RenTec's Medallion fund <br> launched, <br> later to become <br> the most successful hedge fund in history",
  "Heston & Dupire <br> introduce stochastic volatility models",
  "Jump diffusion <br> models introduced",
  "SEC<br>order US stock<br>exchanges to be decimalised",
  "Flash Crash <br> (Markets drop 10% in a matter of minutes)",
  "Basel III <br> requires to periodical estimate <br> counterparty risk of complex derivatives",
  "Quantum computing <br> is proposed by Renbart et. al <br> for derivative pricing",
  "RenTex's Medallion <br> Fund average 66% annual return <br> over last 30 years",
  "CME Smart Stream <br> launched offering real-time <br> cloud-based market data")->financeMs
  

c("Professor JohnMcCarthy (MIT) <br> suggested computing will be sold as a utility",
  "IBM <br> virtualised operating systems",
  "ARPANET <br> launched by US Advanced <br> Research Project Agency <br> connect 4 university computer systems",
  "100,000 <br> computers on Internet",
  "World Wide Web <br> lanuched with 1 million computers on Net",
  "Cloud Computing <br> as a concept <br> introduced in Compaq report",
  "Amazon (AWS) <br> launched as first public cloud service",
  "Big Data <br> as a concept <br> introduced by the <br>OpenNebula research project launched",
  "Elastic Computing (EC2) <br> launched by Amazon",
  "Dropbox <br> launch cloud storage",
  "Microsoft <br> launch Azure cloud computing",
  "DigitalOcean <br> Droplets launched",
  "Real-time <br> streaming data on AWS",
  "Machine learning <br> sold as a service in the Cloud",
  "Massive data-center <br> under the Altantic ocean <br> launched by Microsoft",
  "Google TPU's <br> (Tensor Processing Units) <br> avaliable on the cloud, <br> introducing tensor-based mathematical to public")->cloudMs            
content<-c(financeMs,cloudMs)            
start   = paste0(c("1952","1971","1972","1973","1982","1983","1988","1993","2000","2001","2010","2013","2017","2018","2019",
                   "1961","1967","1969","1988","1991","1996","2002","2005","2006","2007","2010","2012","2013","2015","2018","2019"),"-01-01")

i<-length(financeMs)
j=length(cloudMs)

timeline_data<-data.frame(
  event =str_wrap(content,width=5),
  start=start,
  end=start,
  group=c(rep("Finance",i),rep("Cloud <br> computing",j)),
  fontcolor=c(rep("",i),rep("blue",j)))

p<-hc_vistime(timeline_data,col.color = "fontcolor",title = "Figure 1: Timeline of landscape events in computational finance and cloud computing")
library(highcharter)

thm <- hc_theme(
  colors = c("red", "green", "blue"),
  chart = list(
    backgroundColor = "lightgray"),
  title = list(
    style = list(
      color = "#333333",
      fontFamily = "Shadows Into Light",
      fontSize=25)),
  subtitle = list(
    style = list(
      color = "#666666",
      fontFamily = "Shadows Into Light")),
  legend = list(
    itemStyle = list(
      fontFamily = "Tangerine",
      color = "black"),
    itemHoverStyle = list(
      color = "gray"
    )
  )
)

p %>%
  hc_add_theme(thm)
```

<figurecaption> [@Varghese2019] provides the source of the cloud computing timeline, while the timeline for computational finance events is the authors own calculations. <figurecaption/>

## What is financial machine learning?
Machine learning has been adopted at pace in many real world applications, but has been slow to develop in areas of scientific research, especially financial research where traditional econometric techniques dominate.  Leading econometricians argue this is due to a clashing culture, where some financial economist argues the ontological differences in econometrics and machine learning are intractable [@Athey2019]. This naive comparison highlights the epistemological challenges that are faced by computer age statistical inference of the rapid expansion of algorithmic development[@Efron2016]. Financial machine learning is a sub field in its infancy in the academic literature, which is attempting to reconcile the large differences between econometrics and machine learning.

Machine learning can be defined as a branch of nonparametric statistics mixing, statistical learning, computer science and optimisation [@Molina2019] where algorithms are made up of three fundamental building blocks

1. A loss function
2. An optimisation criteria
3. A optimisation routine    

Changes in each of these building blocks produces a wide variation of learning algorithms characterising the degree of supervision. From an econometric perspective, these models are biased due to their optimisation of a restricted objective according to a specific algorithmic methodology and statistical rationale.  On the other hand, Econometrics applies statistics, usually in the form of a regression analysis, to examine relationships. Model designs using a prior hypothesised model based on well journeyed theories, to produce objective (i.e. minimising bias) statistical inference.  

Broadly speaking financial machine learning attempts to resolve three broad conflicts between machine learning and financial economic research[@Lommers2021]:

1. The importance of statistical inference
2. Causality
3. A prior hypotheses and model assumptions

### Statistical inference
Statistical inference is a broad discipline at the intersection of mathematics, empirical science and philoshopy. Since its philosophical beginnings through the publication of the Bayes rule in 1763 (used by early advocates to argue the existence of god) computation has been a traditional bottleneck for applied statistical inference framework and has motivated small sample solutions with strong asymptotic principles. But since has been  to its most recent advances in computation, a traditional bottleneck to statistical applications up to the early 1950s[@Efron2016].

Statistical inference is the bedrock of econometrics, while the main focus of machine learning is prediction. In traditional econometrics, models are build to learn statistical information and uncertainty about the parameters of the underlying data generating process, using an a prior probability model of the data generating process, with a proven theoretical track record under certain assumptions. In contrasts, machine learning models the focus is on prediction output, where the data generated process is generally undefined, with the goal of algorithmically optimisation models to fit the underlying data generating process as well as possible [@Lommers2021]. [@Efron2016] summaries this well in their definition of computer age statistical inference 

<blockquote>
 Very broadly speaking, algorithms are what statistician do while inference says why they do them.  The efflorescence of ambitious algorithms has forced an evolution (though not a revolution) in inference, the theories by which statisticians choose among competing methods. 
</blockquote>

*The boundary between econometrics and ML is subject to debate. Some methods fall squarely into one or the other camps, but many are used in both. For example, the bootstrap method can be used for statistical inference but also serves as the basis for ensemble methods, such as the Random Forest algorithm. Econometrics requires us to choose a model that incorporates our knowledge of the economic system, and ML requires us to choose a predictive algorithm by relying on its empirical capabilities. Justification for an inference model typically rests on whether we feel it adequately captures the essence of the system. The choice of pattern-learning algorithms often depends on measures of past performance in similar scenarios. Inference and ML are complementary in pointing us to economically meaningful conclusions.*

## Cultural Clashes
Over 20 years ago the Berkeley statistician, Leo Breiman, lambasted the statistical community for their dogmatic approaches, in the face of emerging successes of algorithmic approaches to statistical science. He framed his argue and a culture problem where 

>...the statistical community has been committed almost exclusively to data models...where one assumes that the data are generated by a given stochastic data model. [@Breiman2001]

For the most part the statistical community has now accepted Machine learning as a standard part of a statistical science, with graduate level standards incorporating ML techniques alongside the traditional statistical approaches [@Hastie2009;@Efron2016] and leading statisticians exposing their benefits for enhancing scientific discovery [@Spiegelhalter2019].

While the statistics community has move on, the economics and econometrics community has been much slower to depart from stochastic data generating models as consistency, Normality and efficiency.  ML approaches do not naturally deliver these theoretical properties but leading econometricians argue that if econometrics is to remain relevant for students a balance most be struck between *using data to solve problems*^[This is framing econometrics as decision making under uncertainty[@Dreze1972;@Chamberlain2000;Chamberlain2020]]
while perserving the strengths of applied econometrics [@Athey2019].  Encouragingly, there has been some major advances in theoretical results of the type reported in econometric  [@Athey2017;@Wager2017;Athey2019a;Wagner2019b] and applied statistics papers [@Zuo2021;@Apley2020]


# Computing environment
To understand more about the computational foundations for students we borrow from the extant statistical education literature [@Kaplan2007; @Cetinkaya-Rundel2018].  Much like teaching statistics/data science quantitative finance has two interconnected goals:
1. Get students to do something interesting with data (and code) within the first ten minutes of the first class.
2. Get students to think about computation as an integral part of the quantitative finance curriculum.

An common solution is to use computing labs to facilitate computation exercises.  The downside is that instructors usually do not have administrative access and therefore struggle accomplish even the basic maintenance tasks.  Furthermore, this usually leads to common environment for all courses, rather than specialised set-ups for more enhanced student computational needs.  Finally, to achieve the second goal requires active engagement of computation for all contact time. 

We use a web browser-based solutions, RStudio’s Teams, to provide a frictionless student experience in both lectures and lab sessions. The Workbench product (formerly RStudio server pro) is professional web server software, where facilitates interoperable computation integrating R and Python leveraging support for Jupyter, VSCode, and RStudio integrated development enviroments (IDEs) [@RStudio2021].

# Some background
## Why R and Python?
R and Python are the two leading languages used in industry for data analysis. Thus, to best prepare students to be competitive and perform on the job market, we made the explicit decision to teach both languages on the MSc quantitative finance.  These languages are also infiltrating academia, although there are some notable holdouts where econometrics is taught using commercial graphical user interfaces(GUI).  Proponents of GUI-based econometrics teaching argue that teaching statistical concepts is less intimidating to beginners when using a point-and-click approach compared to command line methods. In argument goes that with the latter teaching programming and statistics in tandem creates too much friction for students.

In my experience such convenience is only possible by removing data analysis from the course content and providing students with tidy, rectangular data.  But for modern financial data analytics this approach is a disservice to students.  Furthermore, point-and-click approaches requires a bespoke student user manual that can run to [40-plus pages](https://github.com/barryquinn1/FMLmaterial/blob/27d8094fee39fa0284d3a0bfc10e38dcd3bebcac/Introducing%20Stata.pdf). We argue this is a considerable learning curve for the novice student which isn’t generalisable to other analytics workflows.  In general, using GUI *copy and paste* workflow can actually increase studen frictions, is more error prone, and harder to debug , and most importantly disconnects the logical link between computing from financial analytics[@Baumer2014].  

### Teaching statistics by simulation
A meaningful understand of econometrics requires more than reading and regurgitating textbook material.  The false discovery crisis in finance suggests that applying out-of-the-box econometric models to observational data also fails to provide a deep understanding of the chasm between econometric model abstractions and the impermanence of the statistical features of financial data [@De_Prado2019].  

## Why RStudio Teams?
Figure 1 visualises the components that make up the bundle of softwares. 

<img src=“imps/Team.png” align=“center”> 

RStudio describe this product as follows: 

>RStudio Team is a bundle of RStudio’s enterprise-grade professional software for scaling data science analytic work across your team, sharing data science results with your key stakeholders, and managing R and Python packages. RStudio Team includes RStudio Workbench, RStudio Package Manager, and RStudio Connect.
RStudio Team offers convenience, simplicity, and savings to organizations using R, Python and RStudio at scale. - [@RStudioT2021]

Teams is a enterprise-grade set-up offered free of charge for academic teaching.  For academic budgets, this is a significant saving, typically between $15,000 to $20,000.  The School’s budget can then focus on purchasing an agile computing infrastructure.  

For teaching computation, the IDE is the most important tool in this bundle. The Workbench product comes with Jupyter (notebook and lab) and RStudio native IDE which provide a powerful interface which helps to flatten the learning curve in command line teaching. It has a series of panes to view data, files, and plots interactively. Additionally, since it is a full fledged IDE, it also features integrated help, syntax highlighting, and context-aware tab completion.

Students access the RStudio IDE through a centralised RStudio server instance, which allows us to provide students with uniform computing environments. Additionally, RStudio’s direct integration with other critically important tools for teaching computing best practices and reproducible research, some
of which we discuss in Sections 3.1 and 3.2, also influenced our decision for making it central in our toolkit.

In 

It should be noted that we do not want to completely dissuade students from downloading and installing R and RStudio locally, we just do not want it to be a prerequisite for getting started. We have found that teaching personal setup is best done progressively throughout a semester, usually via one-on-one interactions during office hours or after class. Our goal is that all students will be able to continue using R even if they no longer have access to departmental resources.

* Solving the **but it worked on my machine?** problem

