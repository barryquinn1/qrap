@ARTICLE{Mahdavi2020,
  title     = "It's all about data: How to make good decisions in a world awash
               with information",
  author    = "Mahdavi, Mehrzad and Kazemi, Hossein",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  volume    =  2,
  number    =  2,
  pages     = "8--16",
  month     =  apr,
  year      =  2020,
  language  = "en"
}


@ARTICLE{Breiman2001,
  title    = "Statistical Modeling: The Two Cultures (with comments and a
              rejoinder by the author)",
  author   = "Breiman, Leo",
  abstract = "There are two cultures in the use of statistical modeling to
              reach conclusions from data. One assumes that the data are
              generated by a given stochastic data model. The other uses
              algorithmic models and treats the data mechanism as unknown. The
              statistical community has been committed to the almost exclusive
              use of data models. This commitment has led to irrelevant theory,
              questionable conclusions, and has kept statisticians from working
              on a large range of interesting current problems. Algorithmic
              modeling, both in theory and practice, has developed rapidly in
              fields outside statistics. It can be used both on large complex
              data sets and as a more accurate and informative alternative to
              data modeling on smaller data sets. If our goal as a field is to
              use data to solve problems, then we need to move away from
              exclusive dependence on data models and adopt a more diverse set
              of tools.",
  journal  = "Statistical Science",
  volume   =  16,
  number   =  3,
  pages    = "199--231",
  month    =  aug,
  year     =  2001,
  language = "en"
}


@MISC{RStudioW2021,
  title        = "{RStudio} Workbench",
  year          = 2021,
  abstract     = "Build great data science products",
  howpublished = "\url{https://www.rstudio.com/products/workbench/}",
  note         = "Accessed: 2021-7-19"
}

@MISC{RStudioT2021,
  title        = “{RStudio} Workbench”,
  year          = 2021,
  abstract     = “RStudio Team is a bundle of RStudio’s enterprise-grade professional software for scaling data science analytic work across your team, sharing data science results with your key stakeholders, and managing R and Python packages. RStudio Team includes RStudio Workbench, RStudio Package Manager, and RStudio Connect.RStudio Team offers convenience, simplicity, and savings to organizations using R, Python and RStudio at scale.”,
  howpublished = “\url{https://www.rstudio.com/products/team/}”,
  note         = “”
}
@INCOLLECTION{Reisinger2018,
  title     = "Finite difference methods for medium-and high-dimensional
               derivative pricing {PDEs}",
  booktitle = "{High-Performance} Computing in Finance",
  author    = "Reisinger, Christoph and Wissmann, Rasmus",
  publisher = "Chapman and Hall/CRC",
  pages     = "175--195",
  year      =  2018
}

@ARTICLE{Cesa2017,
  title     = "A brief history of quantitative finance",
  author    = "Cesa, Mauro",
  abstract  = "In this introductory paper to the issue, I will travel through
               the history of how quantitative finance has developed and
               reached its current status, what problems it is called to
               address, and how they differ from those of the pre-crisis world.",
  journal   = "Probability, Uncertainty and Quantitative Risk",
  publisher = "SpringerOpen",
  volume    =  2,
  number    =  1,
  pages     = "1--16",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Lommers2021,
  title     = "Confronting machine learning with financial research",
  author    = "Lommers, Kristof and Harzli, Ouns El and Kim, Jack",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.068",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Cetinkaya-Rundel2018,
  title     = "Infrastructure and tools for teaching computing throughout the
               statistical curriculum",
  author    = "{\c C}etinkaya-Rundel, Mine and Rundel, Colin",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  72,
  number    =  1,
  pages     = "58--65",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Merton1973,
  title     = "Theory of Rational Option Pricing",
  author    = "Merton, Robert C",
  abstract  = "[The long history of the theory of option pricing began in 1900
               when the French mathematician Louis Bachelier deduced an option
               pricing formula based on the assumption that stock prices follow
               a Brownian motion with zero drift. Since that time, numerous
               researchers have contributed to the theory. The present paper
               begins by deducing a set of restrictions on option pricing
               formulas from the assumption that investors prefer more to less.
               These restrictions are necessary conditions for a formula to be
               consistent with a rational pricing theory. Attention is given to
               the problems created when dividends are paid on the underlying
               common stock and when the terms of the option contract can be
               changed explicitly by a change in exercise price or implicitly
               by a shift in the investment or capital structure policy of the
               firm. Since the deduced restrictions are not sufficient to
               uniquely determine an option pricing formula, additional
               assumptions are introduced to examine and extend the seminal
               Black-Scholes theory of option pricing. Explicit formulas for
               pricing both call and put options as well as for warrants and
               the new ``down-and-out'' option are derived. The effects of
               dividends and call provisions on the warrant price are examined.
               The possibilities for further extension of the theory to the
               pricing of corporate liabilities are discussed.]",
  journal   = "The Bell Journal of Economics and Management Science",
  publisher = "[Wiley, RAND Corporation]",
  volume    =  4,
  number    =  1,
  pages     = "141--183",
  year      =  1973
}

@ARTICLE{Black1973,
  title     = "The Pricing of Options and Corporate Liabilities",
  author    = "Black, Fischer and Scholes, Myron",
  abstract  = "If options are correctly priced in the market, it should not be
               possible to make sure profits by creating portfolios of long and
               short positions in options and their underlying stocks. Using
               this principle, a theoretical valuation formula for options is
               derived. Since almost all corporate liabilities can be viewed as
               combinations of options, the formula and the analysis that led
               to it are also applicable to corporate liabilities such as
               common stock, corporate bonds, and warrants. In particular, the
               formula can be used to derive the discount that should be
               applied to a corporate bond because of the possibility of
               default.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  81,
  number    =  3,
  pages     = "637--654",
  month     =  may,
  year      =  1973
}

@article{Baumer2014,
  title={R Markdown: Integrating A Reproducible Analysis Tool into Introductory Statistics},
  author={B. Baumer and Mine Çetinkaya-Rundel and Andrew Bray and Linda Loi and N. Horton},
  journal={arXiv: Other Statistics},
  year={2014}
}

@article{Kaplan2007,
  title    = {Computing and Introductory Statistics},
  author   = {Kaplan, Daniel},
  abstract = {Author(s): Kaplan, Daniel | Abstract: Much of the computing that
              students do in introductory statistics courses is based on
              techniques that were developed before computing became
              inexpensive and ubiquitous. Now that computing is readily
              available to all students, instructors can change the way we
              teach statistical concepts. This article describes computational
              ideas that can support teaching George Cobb's Three Rs of
              statistical inference: Randomize, Repeat, Reject.},
  journal  = {Technology Innovations in Statistics Education},
  volume   =  {1},
  number   =  {1},
  month    =  {oct},
  year     =  { 2007}
}

@ARTICLE{Molina2019,
  title     = "Machine Learning for Sociology",
  author    = "Molina, Mario and Garip, Filiz",
  abstract  = "Machine learning is a field at the intersection of statistics
               and computer science that uses algorithms to extract information
               and knowledge from data. Its applications increasingly find
               their way into economics, political science, and sociology. We
               offer a brief introduction to this vast toolbox and illustrate
               its current uses in the social sciences, including distilling
               measures from new data sources, such as text and images;
               characterizing population heterogeneity; improving causal
               inference; and offering predictions to aid policy decisions and
               theory development. We argue that, in addition to serving
               similar purposes in sociology, machine learning tools can speak
               to long-standing questions on the limitations of the linear
               modeling framework, the criteria for evaluating empirical
               findings, transparency around the context of discovery, and the
               epistemological core of the discipline.",
  journal   = "Annu. Rev. Sociol.",
  publisher = "Annual Reviews",
  volume    =  45,
  number    =  1,
  pages     = "27--45",
  month     =  jul,
  year      =  2019
}

@ARTICLE{Athey2019,
  title     = "Machine Learning Methods That Economists Should Know About",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "We discuss the relevance of the recent machine learning (ML)
               literature for economics and econometrics. First we discuss the
               differences in goals, methods, and settings between the ML
               literature and the traditional econometrics and statistics
               literatures. Then we discuss some specific methods from the ML
               literature that we view as important for empirical researchers
               in economics. These include supervised learning methods for
               regression and classification, unsupervised learning methods,
               and matrix completion methods. Finally, we highlight newly
               developed methods at the intersection of ML and econometrics
               that typically perform better than either off-the-shelf ML or
               more traditional econometric methods when applied to particular
               classes of problems, including causal inference for average
               treatment effects, optimal policy estimation, and estimation of
               the counterfactual effect of price changes in consumer choice
               models.",
  journal   = "Annu. Rev. Econom.",
  publisher = "Annual Reviews",
  volume    =  11,
  number    =  1,
  pages     = "685--725",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@BOOK{James2013,
  title     = "An Introduction to Statistical Learning: with Applications in
               {R}",
  author    = "James, Gareth and Witten, Daniela and Hastie, Trevor and
               Tibshirani, Robert",
  abstract  = "An Introduction to Statistical Learning provides an accessible
               overview of the field of statistical learning, an essential
               toolset for making sense of the vast and complex data sets that
               have emerged in fields ranging from biology to finance to
               marketing to astrophysics in the past twenty years. This book
               presents some of the most important modeling and prediction
               techniques, along with relevant applications. Topics include
               linear regression, classification, resampling methods, shrinkage
               approaches, tree-based methods, support vector machines,
               clustering, and more. Color graphics and real-world examples are
               used to illustrate the methods presented. Since the goal of this
               textbook is to facilitate the use of these statistical learning
               techniques by practitioners in science, industry, and other
               fields, each chapter contains a tutorial on implementing the
               analyses and methods presented in R, an extremely popular open
               source statistical software platform.Two of the authors co-wrote
               The Elements of Statistical Learning (Hastie, Tibshirani and
               Friedman, 2nd edition 2009), a popular reference book for
               statistics and machine learning researchers. An Introduction to
               Statistical Learning covers many of the same topics, but at a
               level accessible to a much broader audience. This book is
               targeted at statisticians and non-statisticians alike who wish
               to use cutting-edge statistical learning techniques to analyze
               their data. The text assumes only a previous course in linear
               regression and no knowledge of matrix algebra.",
  publisher = "Springer Science \& Business Media",
  month     =  jun,
  year      =  2013,
  language  = "en"
}

@BOOK{Hastie2009,
  title     = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction, Second Edition",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract  = "During the past decade there has been an explosion in
               computation and information technology. With it have come vast
               amounts of data in a variety of fields such as medicine,
               biology, finance, and marketing. The challenge of understanding
               these data has led to the development of new tools in the field
               of statistics, and spawned new areas such as data mining,
               machine learning, and bioinformatics. Many of these tools have
               common underpinnings but are often expressed with different
               terminology. This book describes the important ideas in these
               areas in a common conceptual framework. While the approach is
               statistical, the emphasis is on concepts rather than
               mathematics. Many examples are given, with a liberal use of
               color graphics. It is a valuable resource for statisticians and
               anyone interested in data mining in science or industry. The
               book's coverage is broad, from supervised learning (prediction)
               to unsupervised learning. The many topics include neural
               networks, support vector machines, classification trees and
               boosting---the first comprehensive treatment of this topic in
               any book. This major new edition features many topics not
               covered in the original, including graphical models, random
               forests, ensemble methods, least angle regression \& path
               algorithms for the lasso, non-negative matrix factorization, and
               spectral clustering. There is also a chapter on methods for
               ``wide'' data (p bigger than n), including multiple testing and
               false discovery rates. Trevor Hastie, Robert Tibshirani, and
               Jerome Friedman are professors of statistics at Stanford
               University. They are prominent researchers in this area: Hastie
               and Tibshirani developed generalized additive models and wrote a
               popular book of that title. Hastie co-developed much of the
               statistical modeling software and environment in R/S-PLUS and
               invented principal curves and surfaces. Tibshirani proposed the
               lasso and is co-author of the very successful An Introduction to
               the Bootstrap. Friedman is the co-inventor of many data-mining
               tools including CART, MARS, projection pursuit and gradient
               boosting.",
  publisher = "Springer Science \& Business Media",
  month     =  aug,
  year      =  2009,
  language  = "en"
}


@BOOK{Efron2016,
  title     = "Computer Age Statistical Inference",
  author    = "Efron, Bradley and Hastie, Trevor",
  abstract  = "The twenty-first century has seen a breathtaking expansion of
               statistical methodology, both in scope and in influence. 'Big
               data', 'data science', and 'machine learning' have become
               familiar terms in the news, as statistical methods are brought
               to bear upon the enormous data sets of modern science and
               commerce. How did we get here? And where are we going? This book
               takes us on an exhilarating journey through the revolution in
               data analysis following the introduction of electronic
               computation in the 1950s. Beginning with classical inferential
               theories - Bayesian, frequentist, Fisherian - individual
               chapters take up a series of influential topics: survival
               analysis, logistic regression, empirical Bayes, the jackknife
               and bootstrap, random forests, neural networks, Markov chain
               Monte Carlo, inference after model selection, and dozens more.
               The distinctly modern approach integrates methodology and
               algorithms with statistical inference. The book ends with
               speculation on the future direction of statistics and data
               science.",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2016,
  language  = "en"
}

@MISC{Dreze1972,
  title   = "Econometrics and Decision Theory",
  author  = "Dreze, Jacques H",
  journal = "Econometrica",
  volume  =  40,
  number  =  1,
  pages   = "1",
  year    =  1972
}

@ARTICLE{Chamberlain2020,
  title     = "Robust Decision Theory and Econometrics",
  author    = "Chamberlain, Gary",
  abstract  = "This review uses the empirical analysis of portfolio choice to
               illustrate econometric issues that arise in decision problems.
               Subjective expected utility (SEU) can provide normative guidance
               to an investor making a portfolio choice. The investor, however,
               may have doubts on the specification of the distribution and may
               seek a decision theory that is less sensitive to the
               specification. I consider three such theories: maxmin expected
               utility, variational preferences (including multiplier and
               divergence preferences and the associated constraint
               preferences), and smooth ambiguity preferences. I use a simple
               two-period model to illustrate their application. Normative
               empirical work on portfolio choice is mainly in the SEU
               framework, and bringing in ideas from robust decision theory may
               be fruitful.",
  journal   = "Annu. Rev. Econom.",
  publisher = "Annual Reviews",
  volume    =  12,
  number    =  1,
  pages     = "239--271",
  month     =  aug,
  year      =  2020
}


@ARTICLE{Zuo2021,
  title     = "Variable selection with second-generation P-values",
  author    = "Zuo, Yi and Stewart, Thomas G and Blume, Jeffrey D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Apley2020,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}


@ARTICLE{Athey2019a,
  title     = "Generalized random forests",
  author    = "Athey, Susan and Tibshirani, Julie and Wager, Stefan",
  abstract  = "We propose generalized random forests, a method for
               nonparametric statistical estimation based on random forests
               (Breiman [Mach. Learn. 45 (2001) 5--32]) that can be used to fit
               any quantity of interest identified as the solution to a set of
               local moment equations. Following the literature on local
               maximum likelihood estimation, our method considers a weighted
               set of nearby training examples; however, instead of using
               classical kernel weighting functions that are prone to a strong
               curse of dimensionality, we use an adaptive weighting function
               derived from a forest designed to express heterogeneity in the
               specified quantity of interest. We propose a flexible,
               computationally efficient algorithm for growing generalized
               random forests, develop a large sample theory for our method
               showing that our estimates are consistent and asymptotically
               Gaussian and provide an estimator for their asymptotic variance
               that enables valid confidence intervals. We use our approach to
               develop new methods for three statistical tasks: nonparametric
               quantile regression, conditional average partial effect
               estimation and heterogeneous treatment effect estimation via
               instrumental variables. A software implementation, grf for R and
               C++, is available from CRAN.",
  journal   = "aos",
  publisher = "Institute of Mathematical Statistics",
  volume    =  47,
  number    =  2,
  pages     = "1148--1178",
  month     =  apr,
  year      =  2019,
  keywords  = "62G05; Asymptotic theory; Causal inference; instrumental
               variable;",
  language  = "en"
}

@ARTICLE{Athey2017,
  title         = "Policy Learning with Observational Data",
  author        = "Athey, Susan and Wager, Stefan",
  abstract      = "In many areas, practitioners seek to use observational data
                   to learn a treatment assignment policy that satisfies
                   application-specific constraints, such as budget, fairness,
                   simplicity, or other functional form constraints. For
                   example, policies may be restricted to take the form of
                   decision trees based on a limited set of easily observable
                   individual characteristics. We propose a new approach to
                   this problem motivated by the theory of semiparametrically
                   efficient estimation. Our method can be used to optimize
                   either binary treatments or infinitesimal nudges to
                   continuous treatments, and can leverage observational data
                   where causal effects are identified using a variety of
                   strategies, including selection on observables and
                   instrumental variables. Given a doubly robust estimator of
                   the causal effect of assigning everyone to treatment, we
                   develop an algorithm for choosing whom to treat, and
                   establish strong guarantees for the asymptotic utilitarian
                   regret of the resulting policy.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1702.02896"
}

@ARTICLE{De_Prado2019,
  title     = "A Data Science Solution to the {Multiple-Testing} Crisis in
               Financial Research",
  author    = "de Prado, Marcos L{\'o}pez",
  journal   = "The Journal of Financial Data Science",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  1,
  number    =  1,
  pages     = "99--110",
  year      =  2019
}


@ARTICLE{Athey2019b,
  title         = "Ensemble Methods for Causal Effects in Panel Data Settings",
  author        = "Athey, Susan and Bayati, Mohsen and Imbens, Guido and Qu,
                   Zhaonan",
  abstract      = "This paper studies a panel data setting where the goal is to
                   estimate causal effects of an intervention by predicting the
                   counterfactual values of outcomes for treated units, had
                   they not received the treatment. Several approaches have
                   been proposed for this problem, including regression
                   methods, synthetic control methods and matrix completion
                   methods. This paper considers an ensemble approach, and
                   shows that it performs better than any of the individual
                   methods in several economic datasets. Matrix completion
                   methods are often given the most weight by the ensemble, but
                   this clearly depends on the setting. We argue that ensemble
                   methods present a fruitful direction for further research in
                   the causal panel data setting.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "econ.EM",
  eprint        = "1903.10079"
}

@ARTICLE{Wager2017,
  title     = "Estimation and Inference of Heterogeneous Treatment Effects
               using Random Forests",
  author    = "Wager, Stefan and Athey, Susan",
  abstract  = "ABSTRACTMany scientific and engineering challenges?ranging from
               personalized medicine to customized marketing
               recommendations?require an understanding of treatment effect
               heterogeneity. In this article, we develop a nonparametric
               causal forest for estimating heterogeneous treatment effects
               that extends Breiman?s widely used random forest algorithm. In
               the potential outcomes framework with unconfoundedness, we show
               that causal forests are pointwise consistent for the true
               treatment effect and have an asymptotically Gaussian and
               centered sampling distribution. We also discuss a practical
               method for constructing asymptotic confidence intervals for the
               true treatment effect that are centered at the causal forest
               estimates. Our theoretical results rely on a generic Gaussian
               theory for a large family of random forest algorithms. To our
               knowledge, this is the first set of results that allows any type
               of random forest, including classification and regression
               forests, to be used for provably valid statistical inference. In
               experiments, we find causal forests to be substantially more
               powerful than classical methods based on nearest-neighbor
               matching, especially in the presence of irrelevant covariates.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  pages     = "1--15",
  month     =  apr,
  year      =  2017
}

@ARTICLE{Chamberlain2000,
  title    = "Econometrics and decision theory",
  author   = "Chamberlain, Gary",
  abstract = "The paper considers the role of econometrics in decision making
              under uncertainty. This leads to a focus on predictive
              distributions. The decision maker's subjective distribution is
              only partly specified; it belongs to a set S of distributions. S
              can also be regarded as a set of plausible data-generating
              processes. Criteria are needed to evaluate procedures for
              constructing predictive distributions. We use risk robustness and
              minimax regret risk relative to S. To obtain procedures for
              constructing predictive distributions, we use Bayes procedures
              based on parametric models with approximate prior distributions.
              The priors are nested, with a first stage that incorporates
              qualitative information such as exchangeability, and a second
              stage that is quite diffuse. Special points in the parameter
              space, such as boundary points, can be accommodated with
              second-stage priors that have one or more mass points but are
              otherwise quite diffuse. An application of these ideas is
              presented, motivated by an individual's consumption decision. The
              problem is to construct a distribution for that individual's
              future earnings, based on his earnings history and on a
              longitudinal data set that provides earnings histories for a
              sample of individuals.",
  journal  = "J. Econom.",
  volume   =  95,
  number   =  2,
  pages    = "255--283",
  month    =  apr,
  year     =  2000,
  keywords = "Expected utility; Predictive distribution; Risk robustness;
              Minimax regret; Bayes procedure; Longitudinal data"
}


@BOOK{Spiegelhalter2019,
  title     = "The Art of Statistics: Learning from Data",
  author    = "Spiegelhalter, David",
  abstract  = "'A statistical national treasure' Jeremy Vine, BBC Radio
               2'Required reading for all politicians, journalists, medics and
               anyone who tries to influence people (or is influenced) by
               statistics. A tour de force' Popular ScienceDo busier hospitals
               have higher survival rates? How many trees are there on the
               planet? Why do old men have big ears? David Spiegelhalter
               reveals the answers to these and many other questions -
               questions that can only be addressed using statistical
               science.Statistics has played a leading role in our scientific
               understanding of the world for centuries, yet we are all
               familiar with the way statistical claims can be sensationalised,
               particularly in the media. In the age of big data, as data
               science becomes established as a discipline, a basic grasp of
               statistical literacy is more important than ever. In The Art of
               Statistics, David Spiegelhalter guides the reader through the
               essential principles we need in order to derive knowledge from
               data. Drawing on real world problems to introduce conceptual
               issues, he shows us how statistics can help us determine the
               luckiest passenger on the Titanic, whether serial killer Harold
               Shipman could have been caught earlier, and if screening for
               ovarian cancer is beneficial. 'Shines a light on how we can use
               the ever-growing deluge of data to improve our understanding of
               the world' Nature",
  publisher = "Penguin UK",
  month     =  mar,
  year      =  2019,
  language  = "en"
}


@ARTICLE{Varghese2019,
  title         = "Cloud Futurology",
  author        = "Varghese, Blesson and Leitner, Philipp and Ray, Suprio and
                   Chard, Kyle and Barker, Adam and Elkhatib, Yehia and Herry,
                   Herry and Hong, Cheol-Ho and Singer, Jeremy and Tso, Fung Po
                   and Yoneki, Eiko and Zhani, Mohamed-Faten",
  abstract      = "The Cloud has become integral to most Internet-based
                   applications and user gadgets. This article provides a brief
                   history of the Cloud and presents a researcher's view of the
                   prospects for innovating at the infrastructure, middleware,
                   and application and delivery levels of the already crowded
                   Cloud computing stack.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "1902.03656"
}

@ARTICLE{Lommers2021,
  title     = "Confronting machine learning with financial research",
  author    = "Lommers, Kristof and Harzli, Ouns El and Kim, Jack",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.068",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

