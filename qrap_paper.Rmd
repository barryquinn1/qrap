---
title: "Teaching statistical inference in the age of financial technology"
authors:
  - name: Barry Quinn
    thanks: A special thanks to Dr Alan Hanna for his insightful comments.
    department: Queens Management School
    affiliation: Queen's University Belfast
    location: Belfast
    email: b.quinn@qub.ac.uk
abstract: |
  This paper seeks to understand the challenges of teaching statistical inference in finance in the computer age.  We argue that the unstoppable algorithmic tranformation of financial services, and the nascent field of financial machine learning provide an opportunity to redesign finance programmes for the age of financial technology. We argue it is time for a rethink how we can extract reliable statistical inference from financial data given proliferation of computing, *Big financial data*, and the unstoppable algorithmisation the finance industry. The paper begins by agnostically profiling the modelling paradigm choice. Next we establish the developments in statistical inference in the computer age specific to finance. Finally, we consider the idea of placing computation as a central tennent in finance curicullum, and discuss the infrastructure and tools involved.  We illustrate a use case where the infrastrcture is on-boarded in a cloud computing suite with enterprise-level server software. We are not arguing that finance is computation, rather that by placing computation as a frictionless part of the curicullum, students can engage with the full suite of state-of-the-art inferential tools avaliable to financial data science practicioners.    
keywords:
  - Finance education
  - Financial technology
  - Statistical inference
  - Financial data science
  - Financial machine learning
  - Econometrics
  - Cloud computing
  - Employability 
bibliography: biblio.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage[table]{xcolor}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,fig.align = "center",warning = FALSE, comment = FALSE,cache = TRUE)
pacman::p_load('flextable','tidyverse')
```
# Introduction

The algorithmic transformation of financial services has seen the financial technology (FinTech) industry surge from the sidelines to the mainstream^[*A progress report on fintech's record-breaking year* by Nicholas Megaw August 2021. https://www.ft.com/content/89ea3d5d-cd29-46ec-88f1-67729b09a7c2?shareType=nongift].  This is especially true in the United Kingdom, where FinTech is viewed as **a permanent technology revolution that is changing the way we do finance**^[Kalifa Review of UK FinTech 2021, https://www.gov.uk/government/publications/the-kalifa-review-of-uk-fintech].  FinTech is multidisciplinary and is moving from the technological era of *social, mobile, analytics and cloud*(SMAC) to a future where *distributed ledger technology, artificial intelligence, extended reality, Quantum computing*(DARQ) technologies are the next differentiator combination. For students to remain relevant in this fast-paced world, the use of computation to enable reliable inference most play a central role in curiccula.

While finance is a social science, many parts of modern finance are fundamentally quantitative, with financial practitioners solving practical problems using innovative technologies.  Furthermore, the rise of big and alternative data combined with the exponential growth of AI and financial data science has created new opportunities in the financial sector. AI and machine learning applications are now widespread and include innovations in risk management [@Lin2017], portfolio construction [@Jaeger2021], investment banking [@IBC2020] and insurance [@SOA2020]. In short, the *algorithmisation* of finance is unstoppable [@De_Prado2019].
  
While narrow AI, which uses rule-based algorithms, has dominated the fast-paced automation of tasks in financial services, researchers predict the next wave of automation will be the digitising judgement calls [@Lopez_de_Prado2018].  Given that finance professionals have an essential fiduciary duty towards their clients, the rapid growth of artificial intelligence (AI) in finance has highlighted some critical risks around trust, overfitting, lack of interpretability, biased inputs and unethical use of data.  Now more than ever, highly computationally digitally literate finance graduates are needed to balance algorithmic technology developments with sustainability, ethics, bias, and privacy to create *trustworthy* data-driven decisions [@Mahdavi2020].

The UK is leading the way in FinTech innovation and are forging on with a large scale plan post-Brexit. The 2021 Kalifa Review on FinTech sets out an ambitious five point plan to foster and scale UK based FinTech firms.  A central part of this plan is to *upskill*, and *reskill* adults by developing training and courses from high-quality universities. Now more than ever, there are exciting opportunities for computationally literate finance graduates in the UK.

This paper provides an overview of the opportunities and challenges for the finance education curricula in the fast-paced world of financial technology innovations.  We specifically focus on how traditional econometric inference is changing, the emerging field of financial machine learning, and how to embed computation to facilitate a frictionless approach to teaching statistical inference.  We provide an overview of how this has been achieved in the Management School of Queens University Belfast using an enterprise-scale cloud computing infrastructure and a suite of enterprise-level web software.

# Background
## What is financial machine learning (hereafter FML)?
Machine learning (hereafter ML) has been adopted at a pace in many real-world applications but has been slow to develop in areas of scientific research, especially economic analysis, where traditional econometric techniques dominate.  Leading econometricians argue this is due to a clashing culture [@Athey2019a], where some financial economists view the ontological differences in econometrics and machine learning as intractable. This naive comparison highlights the epistemological challenges computer age statistical inference faces in a world of rapid algorithmic development [@Efron2016]. FML is a subfield of AI in its infancy, attempting to reconcile the differences between econometrics and ML.

ML is a  branch of nonparametric statistics mixing statistical learning, computer science and optimisation [@Molina2019], where algorithms have three fundamental building blocks:

1. A loss function;
2. An optimisation criteria;
3. An optimisation routine.    

Changes in each of these building blocks produce a wide variety of learning algorithms characterising the freedom they have to learn patterns in the data.  Broadly speaking, ML algorithms are categorised into unsupervised learning and supervised learning. A classic example of the former is clustering, and the latter is a regression tree. A learning algorithm with no feedback is unsupervised in that the analyst provides no information to guide the learning process. In contrast supervised learning involves feedback in the form of training data which is correctly labelled.  Between these two extremes there are other types of machine learning which are particularly popular in finance.  Reinforcement learning uses partial feedback, in the form of *rewards*, to encourages the desired behaviour without instructing the algorithm precisely [@Dixon2020].  

In the classical sense, ML models are statistically biased.  due to their optimisation of a restricted objective according to a specific algorithmic methodology and statistical rationale.  On the other hand, econometrics applies statistics to a data sample, usually in the form of regression analysis, to examine relationships. The model design uses well-journeyed economic theory to develop an *unobservable* hypothesised model. The asymptotic theory is then relied upon to produce objective statistical inference, which minimises bias, possibly at the expense of increased sampling variation.

FML attempts to reconcile three broad conflicts between ML and econometrics[@Lommers2021]:

1. The importance of statistical inference and modelling paradigm;
2. Causality;
3. An a priori hypotheses and model assumptions.

In what follows we consider each of these conflicts in term.  We begin with the choice of estimation models.

## Modelling paradigm

To move from computation to inference in statistics we most make a estimation choice.  Mathematical statistics is the science of learning from experience that arrives a little at a time [@Efron2016], and using this information to quantify uncertainty and variation[@Spiegelhalter2019]. Unlike many other disciplines in mathematics, there is no unifying theory. Reliable inference from statistics requires careful thought beyond the computing algorithm. The modern financial data scientist is faced with a choice between two abstracting *leaps of faith* in order to go from computation to meaningful inference. Broadly speaking, the theoretical inference paradigms are classical (or frequentist) and bayesian .^[Roughly speaking, frequentists infer meaning by asking themselves *what would I see if I reran the same situation again (and again and again and again,..., ad infinitum, et ultra)?*. On the other hand, bayesians coax a fantastical belief that *they have prior knowledge of situation, encode this in probability, and update this knowledge by learning from a set of observed data points*.  In essence, to generalise from statistical measurement one need to be somewhat of a fantasist.]. The goal of this paper is to present a disinterested perspective on both paradigms, and provide some guidance on when and why each choice is preferred in financial data science problems.  

### Frequentist inference
With its developmental beginnings in 1900, frequentism has grown to dominate 20^th^ century statistical inference in finance. A remarkably potent theory, it was primarily designed to produce maximally efficient statistical analysis using small data collected under strictly controlled conditions.

The name *frequentism* seems to have been suggested by Neyman as a statistical analogue of Richard von Mises' frequentist theory of probability; the connection is made explicit in his 1977 paper, "Frequentist probability and frequentist statistics." "Behaviourism" might have been a more descriptive name since the theory revolves around the long-run behaviour of statistics $t(x)$, but in any case *frequentism* has stuck, replacing the older (sometimes disparaging) term *objectivism*. Neyman's attempt at a complete frequentist theory of statistical inference, *inductive behaviour*, is not much-quoted today but can claim to influence Wald's development of decision theory.

Statistical inference usually begins with the assumption that some probability model has produced the observed data x, in our case, the vector of fake prices measurements $x=(x_1,x_2,..,x_n)$. Let $X=(X_1,X_2,...,X_n)$ indicate n independent draws from a probability distribution F, written:
$$F \to X$$
F is the underlying distribution of possible prices (the model).  The statistician observes a realization $X= x$ of $F \to X$, and the then wishes to infer some property of the unknown distribution F . Suppose the desired property is the expectation of a single random draw X from F, denoted
$$\theta=E_f\left\{ X \right\}$$
Otherwise, there is room for error and *the inferential question is how much error?*.  The estimate $\hat{\theta}$ using some algorithm (simple averaging in this instance).  Importantly, $\hat{\theta}$ is a realisation of $\bf{\Theta}=t(X)$ the output of t(.) applied to some theoretical sample $X$ from $F$.  It follows that frequentist inference focuses on the accuracy of an observed estimate $\hat{\theta}=t(x)$ is the probabilistic accuracy of $\bf{\Theta}=t(X)$ as an estimator of $\theta$.  This proposition contains the powerful idea that $\hat{\theta}$ is just a number, but $\hat{\Theta}$ takes a range of values whose spread can define measures of accuracy.

Bias and variance are familiar examples of frequentist inference.  Defining $\mu$ to be the expectation of $\hat{\Theta}=t(X)$ under the model $F \to X$:
$$\mu=E_F\left\{\hat{\Theta}\right\}$$
The bias attributed to estimate $\hat{\theta}$ of parameter $\theta$ is
$$bias=\mu - \theta$$. The variance attributed to estimate $\hat{\theta}$ of parameter $\theta$ is: 

$$var=E_f\left\{(\hat{\Theta}-\mu)^2\right\}$$

Importantly, what keeps this from being a tautology, and is one of its biggest Bayesian criticisms, is that attribution to the *single number* $\hat{\theta}$ of the probabilistic properties of $\hat{\Theta}$ derived from the model $$F \to X$$.

Commonly, frequentism is defined as *an infinite sequence of future trials.*  We imagine hypothetical datasets $X^{(1)};X^{(2)};X^{(3)}...$ generated by the same mechanism as x providing corresponding values $\hat{\Theta}^{(1)};\hat{\Theta}^{(2)};\hat{\Theta}^{(3)}...$. The frequentist principle is then to attribute for $\hat{\theta}$ the accuracy properties of the ensemble of $\hat{\Theta}$ values. As mentioned above, in essence, frequentists ask themselves, *What would I see if I reran the same situation (and again and again)....?*

In practice, there is an apparent defect in this principle.  It requires the calculation of the properties of the estimators $\bf{\Theta}=t(X)$ obtained from the actual distribution F, even though F is unknown. In practice, frequentism uses a collection of ingenious devices to circumvent the defect, including the plugin principle^[The frequentist accuracy estimate for the mean of x plugs in as an estimate of the variance of a single $X$ draw from $F$ into a formula relating the standard error to this said variance.], Taylor series approximations^[ Statistics more complicated than a simple average can often be related to the plugin formula by local linear approximation sometimes know as the *delta method*.], parametric families and maximum likelihood theory, simulation and the bootstrap^[modern computation has opened up the possibility of numerically implementing the *infinite sequence of future trails* except for the infinite part.], and pivotal statistics^[These are statistics whose distribution does not depend upon the underlying probability distribution $F$. A classic example is Student's two-sample $t$-test statistic]

The popularity of frequentist methods reflects their relatively modest mathematical modelling assumptions: only a probability model F (more exactly a family of probabilities) and an algorithm of choice. Such flexibility has some defects. Primarily, the principle of frequentist correctness does not help with the choice of algorithm. That is frequentist need to find the *best*(optimal) choice of $t(x)$ given model $F$. In the early 1900s, two theories emerged. 
1. Fisher's theory of maximum likelihood: in specific parametric probability models, the MLE is the optimum estimate in terms of the minimum(asymptotic) standard error.
2. Neyman-Pearson lemma provides an optimum hypothesis-testing algorithm.

## Bayesian inference 

The human mind is an inference machine: “It’s getting windy, the sky is darkening, I’d better bring my umbrella with me.”  Unfortunately, it’s not a very dependable machine, especially when weighing complicated choices against past experience.  Bayes’ theorem is a surprisingly simple mathematical guide to accurate inference.  The theorem (or “rule”), now 250 years old, marked the beginning of statistical inference as a serious scientific subject.  It has waxed and waned in influence over the centuries, now waxing again in the service of computer-age algorithms and inference.

Bayesian inference, if not directly opposed to frequentism, is at least orthogonal.  It reveals some worrisome flaws in the frequentist point of view, while at the same time exposing itself to the criticism of dangerous overuse.  The struggle to combine the virtues of the two philosophies has become more acute in an era of massively complicated data sets.  Here we will review some basic Bayesian ideas and the ways they impinge on frequentism.  A Bayesian statistical model can be thought of as a model for learning from data.  Machine learning lays comfortably within this definition.

### The garden of forking data
Modestly, Bayesian inference is really just counting and comparison of possibilities. 
Bayesian inference uses a concept similar to Jorge Luis Borges short story [The Garden of Forking Paths](https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths).  In this book Borges explores all paths, with each decision branching outward into an expanding garden of forking paths. This is the same device that Bayesian inference offers. 

In order to make good inference about what actually happened, it helps to consider everything that could have happened.  A Bayesian analysis is a garden of forking data, in which alternative sequences of events are cultivated.  As we learn about what did happen, some of these alternative sequences are pruned. In the end, what remains is only what is logically consistent with our knowledge.  This approach provides a quantitative ranking of hypotheses, a ranking that is maximally conservative, given the assumptions and data that go into it.   The approach cannot guarantee a correct answer, on large world terms. But it can guarantee the best possible answer, on small world terms, that could be derived from the information fed into it

Suppose there’s a bag, and it contains four marbles.  These marbles come in two colours: blue and white. We know there are four marbles in the bag, but we don’t know how many are of each colour. We do know that there are five possibilities:
```{r possibilities, fig.cap="All the possible draws from a bag containing 2 white and 2 blue"}
library(tidyverse)
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)
d %>% 
  gather() %>% 
  mutate(x = rep(1:4, times = 5),
         possibility = rep(1:5, each = 4)) %>% 
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```

These are the only possibilities consistent with what we know about the contents of the bag.  Call these five possibilities the conjectures.  Our goal is to figure out which of these conjectures is most plausible, given some evidence about the contents of the bag.  We do have some evidence: A sequence of three marbles is pulled from the bag, one at a time, replacing the marble each time and shaking the bag before drawing another marble.   The sequence that emerges is: blue,white,blue in that order. These are the data.  So now let’s plant the garden and see how to use the data to infer what’s in the bag.  Let’s begin by considering just the single conjecture, that the bag contains one blue and three white marbles.  After three draws there is 64 possible paths $(4^3)$ but as we consider each draw from the bag, some of the paths are logically eliminated.

```{r forking_paths, fig.cap="All possible pathways"}
d <-
  tibble(position = c((1:4^1) / 4^0, 
                      (1:4^2) / 4^1, 
                      (1:4^3) / 4^2),
         draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),
         fill     = rep(c("b", "w"), times = c(1, 3)) %>% 
           rep(., times = c(4^0 + 4^1 + 4^2))) %>% 
  mutate(denominator = ifelse(draw == 1, .5,
                              ifelse(draw == 2, .5 / 4,
                                     .5 / 4^2))) %>% 
  mutate(position    = position - denominator)

lines_1 <-
  tibble(x    = rep((1:4), each = 4),
         xend = ((1:4^2) / 4),
         y    = 1,
         yend = 2)  %>% 
  mutate(x    = x - .5,
         xend = xend - .5 / 4^1)

lines_2 <-
  tibble(x    = rep(((1:4^2) / 4), each = 4),
         xend = (1:4^3) / (4^2),
         y    = 2,
         yend = 3) %>% 
  mutate(x    = x - .5 / 4^1,
         xend = xend - .5 / 4^2)
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 4) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  theme(panel.grid      = element_blank(),
        legend.position = "none") +
  coord_polar()

```
]

We can then eliminate the paths inconsistent with the observed sequence.  The first draw turned out to be blue, If you imagine the real data tracing out a path through the garden, it must have passed through the one blue path near the origin.   The second draw from the bag produces a white marble , so three of the paths forking out of the first blue marble remain.  Finally, the third draw is blue.  Visually we can see that by logically eliminated the other paths it leaves a total of three ways for the sequence to appear, assuming the bag contains [blue,white,white,white].

```{r, fig.cap="Eliminating the inconsistent pathway given the data" }
lines_1 <-
  lines_1 %>% 
  mutate(remain = c(rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 3)))

lines_2 <-
  lines_2 %>% 
  mutate(remain = c(rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 12 * 4)))

d <-
  d %>% 
  mutate(remain = c(rep(1:0, times = c(1, 3)),
                    rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 12 * 4))) 

# finally, the plot:
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_point(aes(fill = fill, alpha = remain %>% as.character()),
             shape = 21, size = 4) +
  # it's the alpha parameter that makes elements semitransparent
  scale_alpha_manual(values = c(1/10, 1)) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  theme(panel.grid      = element_blank(),
        legend.position = "none") +
  coord_polar()
```
To summarize, we’ve considered five different conjectures about the contents of the bag, ranging from zero blue marbles to four blue marbles. For each of these conjectures, we’ve counted up how many sequences, paths through the garden of forking data, could potentially produce the observed data,[blue,white,blue].

```{r, conjecture summary, fig.cap="Summary of conjecture"}
n_blue <- function(x){
  rowSums(x == "b")
}

n_white <- function(x){
  rowSums(x == "w")
}

  # for the first four columns, `p_` indexes position
  tibble(p_1 = rep(c("w", "b"), times = c(1, 4)),
         p_2 = rep(c("w", "b"), times = c(2, 3)),
         p_3 = rep(c("w", "b"), times = c(3, 2)),
         p_4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(`draw 1: blue`  = n_blue(.),
         `draw 2: white` = n_white(.),
         `draw 3: blue`  = n_blue(.)) %>% 
  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)->t
t %>%
    knitr::kable() %>%
    kableExtra::kable_paper()
```
Notice that the number of ways to produce the data, for each conjecture, can be computed by first counting the number of paths in each *ring* of the garden and then by multiplying these counts together.  Note that multiplication is just counting condensed.  This point will come up again when we look at the formal representation of Bayesian inference.  We can use these counts to rate the relatively plausibility of each conjecture.  Luckily, there’s a mathematical way to compress all of this. Specifically, we define the updated plausibility of each possible composition of the bag, after seeing the data, as:

Plausibility of [bwww] after seeing bwb $\propto$ ways [bwww] can produce [bwb] $\times$ prior plausibility of [bwww]

Probability can be thought of as plausibility standardise and if we helpfully define p=1/4 (the proportion of marbles that are blue)

```{r, fig.cap="Posterior probabilities"}
t %>% 
  select(p_1:p_4) %>% 
  mutate(p= seq(from = 0, to = 1, by = .25),
         `ways to produce data` = c(0, 3, 8, 9, 0)) %>% 
  mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) %>% 
  knitr::kable() %>%
    kableExtra::kable_paper()
```


A conjectured proportion of blue marbles, p, is usually called a *parameter* value. It’s just a way of indexing possible explanations of the data.  The relative number of ways that a value p can produce the data is usually called  a *likelihood*. It is derived by enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data. The prior plausibility of any specific p is usually called the *prior probability*.  The new, updated plausibility of any specific p is usually called the *posterior* probability

For both Bayesians and frequentists the fundamental unit of statistical inference are probability densities:
$$F=\left\{f_u(x);x \in X,\mu \in \Omega \right\};$$

where x, the observed data, is a point in the sample space X, while unobserved parameter $\mu$ is a point in the parameter space $\Omega$.  A statistician observes x from $f_u(x)$, and infer the value of $\mu$ ^ [ Popular families of distributions include

1. the Normal family

$$f_u(x)=\frac{1}{\sqrt{2 \pi}} e^{-0.5(x-\mu)^2}$$  

useful when we want $X$ and $\Omega$ being on the entire real line $(-\infty,\infty)$

2. Poisson family

$$f_u(x)=e^{-\mu)}\mu^-x/x!$$

useful when $X$ is a nonnegative integer $\left\{0,1,2,...\right\}$ and $\Omega$ is the nonnegative real number line $(0,\infty)$
].  In addition Bayesian inference requires one crucial assumption, the knowledge of a prior density concerning the parameter $g(\mu),\mu \in \Omega$.  $g(\mu)$ represents prior information concerning the parameter $\mu$, available to the statistician $before$ the observation of x.  Exactly what constitutes **prior knowledge** is a crucial and at time contentious question in econometrics.

Roughly speaking, Bayesian inference is about counting probabilities.  We the previous marbles example we seen this.  The rule is a simple exercise in conditional probability.  Formally,  $g(\mu|x)=g(\mu)f_{\mu}(x)/f(x)$, where f(x) is a marginal density (an integral or a sum of discrete where we count up all the possibilities).  In this rule $x$ is fixed at its observed value while $\mu$ varies over $\Omega$.  **This is the opposite of frequentist calculations**.  A memorable restatement of this rule is that the posterior odds ratio is the prior odds ratio time the likelihood ratio.  Formally, this is defined for any two points $\mu_1$ and $\mu_2$ on $\Omega$ as: 

$$\frac{g(\mu_1|x)}{g(\mu_2|x)}=\frac{g(\mu_1)}{g(\mu_1)}\frac{f_{\mu_1}(x)}{f_{\mu_2}(x)}$$


### Comparing modelling paradigms

![](/Users/barry/Dropbox/Teaching/FinancialMachineLearning/ati_infer/img/casi_bayesianvdfrequentist.png){style="center" height='20em' width='20em'}

Bayesians and frequentists start out on the same playing field, a family of probability distributions $f_{\mu}(x)$.  But play the game in orthogonal directions, as indicated schematically in Figure 3.5  Bayesian inference proceeds vertically, with x fixed, according to the posterior distribution $g(u|x)$.  Frequentists reason horizontally, with fixed $\mu$ and x varying.  There are advantages and disadvantages accrue to both strategies.

Bayesian inference requires a prior distribution $g(\mu)$.  When past experience provides $g(\mu)$,there is every good reason to employ Bayes’ theorem.  If not, techniques such as those of [Jeffreys](https://en.wikipedia.org/wiki/Jeffreys_prior) still permit the use of Bayes’ rule, but the results lack the full logical force of the theorem.  The Bayesian’s right to ignore selection bias, for instance, must then be treated with caution.  Frequentism replaces the choice of a prior with the choice of a method, or algorithm, $t(x)$, designed to answer the specific question at hand.  This adds an arbitrary element to the inferential process, and can lead contradictions.  Optimal choice of $t(x)$ reduces arbitrary behavior, but computer-age applications typically move outside the safe waters of classical optimality theory, lending an ad-hoc character to frequentist analyses.

Modern data-analysis problems are often approached via a favored methodology, such as logistic regression or regression tree.  This plays into the methodological orientation of frequentism, which is more flexible than Bayes’ rule in dealing with specific algorithms.  Though one always hopes for a reasonable Bayesian justification for the method at hand.

Having chosen $g(\mu)$ only a single probability distribution $g(\mu|x)$ is in play for Bayesians. Frequentists, by contrast, must struggle to balance the behavior of $t(x)$ over a family of possible distributions, since $\mu$ in Figure 3.5 is unknown.  The growing popularity of Bayesian applications (usually begun with uninformative priors) reflects their simplicity of application and interpretation.The simplicity argument cuts both ways. The Bayesian essentially bets it all on the choice of his or her prior being correct, or at least not harmful.Frequentism takes a more defensive posture, hoping to do well, or at least not poorly, whatever $\mu$ might be.

A Bayesian analysis answers all possible questions at once.  Frequentism focuses on the problem at hand, requiring different estimators for different questions. This is more work, but allows for more intense inspection of particular problems.  The simplicity of the Bayesian approach is especially appealing in dynamic contents, where data arrives sequentially and updating one’s beliefs is a natural practice.  Financial market dynamics are a case in point.  Bayes’ theorem is an excellent tool in general for combining statistical evidence from disparate sources,the closest frequentist analog being maximum likelihood estimation.

In the absence of genuine prior information, a whiff of subjectivity hangs over Bayesian results, even those based on uninformative priors.  Classical frequentism claimed for itself the high ground of scientific objectivity, especially in contentious areas such as drug testing and approval, where skeptics as well as friends hang on the statistical details.Figure 3.5 is soothingly misleading in its schematics: In FML $\mu$ and $x$ are typically be high-dimensional , sometimes very high-dimensional, straining to the breaking point both the frequentist and the Bayesian paradigms. 
Computer-age statistical inference at its most successful combines elements of the two philosophies, as for instance in the empirical Bayes methods or the lasso

There are two potent arrows in the statistician’s philosophical quiver, and faced, say, with 1000 parameters and 1,000,000 data points, there’s no need to go hunting armed with just one of them.


### Teaching statistical inference
Traditional econometrics has favoured the frequentist paradigm, but with the increase in computer power, improvements in Markov Chain Monte Carlo (MCMC) methods, and advances in probabilistic programming bayesian inference more common place in finance. Yet teaching statistical inference in finance is still dominated by frequentism, with bayesian inference a footnote at best.  The perception among some educators is that bayesian statistics are too complex a topic for introductory course, but the human brain is a inference engine which intuitively learns concepts are more intuitive and more easy to teach in an introductory course.  A naive assumption is that these paradigms are competing and in traditional econometrics, the frequentist approach is preferred due to its ability to be more objective.  But objectivity is explicitly linked to the analyst’s confidence in the prior belief that asymptotic properties of frequentist methods hold true. But bayesian inference does require the additional setting to prior probabilities.

bit on Efron



bit on bayesian econometrics in finance Jacquier and Polson

Bayesian econometrics in finance is becoming an increasing important tool for 
So far we have consider only the
At this point, it is important to note that *traditional* approach we discuss in the previous section is based on the frequentist paradigm, but an important alternative that has gain prominence in line with increasing compute power is bayesian econometrics.  


### Statistical inference
Statistical inference is a broad discipline at the intersection of mathematics, empirical science and philosophy. Since its philosophical beginnings through the publication of the Bayes rule in 1763^[Which was used by early advocates to argue the existence of God.], computation has been a traditional bottleneck for applied statistical inference frameworks, motivating small sample solutions with solid asymptotic principles [@Efron2016].  Traditional econometrics retained much of this framework arguable because of the sparsity of observed data realisation of theory. Up until the early 1950s, the computation bottle still dominated small sample solutions in applied statistics.  But as power and accessibility of computing have increased, and statistical theory has developed, statistical inference using machine learning model has become commonplace for applied statisticians^[One notable example is the *bootstrap* a computer-intensive inferential engine that is now ubiquitous in applied statistics.]

Statistical inference is the bedrock of econometrics, while the main focus of ML is prediction. In traditional econometrics, models learn statistical information and uncertainty about the parameters of the *unobservable* data generating process. Their power emanates from an *a priori* probability model under strict assumptions with a proven track record. Armed with this theoretical confidence and using the dominant frequentist approach, econometricians can objectively infer uncertainty and variation characteristics about **how well the data sampled maps to the theoretical data generating process**.  

Econometricians coax validate statistical inference using amenable distributional assumptions and model specifications. The three most important properties in most traditional econometrics models are linearity, additivity and monotonicity. The most important assumption, which is routine overlooking in many textbooks, is **validity**. Andrew Gelman summarises this property as: 

> The data you are analysing should map to the research question you are trying to answer.  This assumption sounds obvious but is often overlooked or ignored because it can be inconvenient.  Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalise to the cases to which it will be applied. - [@Gelman2020]   

These amenable formulations provide a convenient root to statistical significance using p-values [@Lommers2021] but the inherent philosophy of traditional econometric models is incompatible with out-of-sample inference and prediction [@De_Prado2019]; two task which are at the core of the modern finance industry.

In contrast, machine learning models focus on outcome prediction, where the data generated process is generally undefined, with the goal of algorithmically optimising models to fit the underlying data generating process as well as possible [@Lommers2021]. [@Efron2016] summaries this well in their definition of computer age statistical inference:

 > Very broadly speaking, algorithms are what statisticians do, while inference says why they do them. However, the efflorescence of ambitious algorithms has forced an evolution (though not a revolution) in inference, the theories by which statisticians choose among competing methods.

Thus the challenge for modern computationally literate finance graduate is to understand the inferential benefits of machine learning through the lens of financial econometrics. For inference to be convincing, more work must be done on statistical consistency of machine learning models.  For instance, in the area of explainable AI (XAI), great strides have been made on producing statistical consistent and cognitive convincing explanations of the importance of predictors in the ML model [@Barredo_Arrieta2020].  For instance in recent years there have been notable advances. For example second-generation p-values can be included in a penalised regression model to yield tangible advantages for balancing support recovery, parameter estimation, and prediction tasks [@Blume2019; @Zuo2021]. 

### Causality

Identifying causal effects with data has a long and varied history. It's origins span many disciplines, including early statisticians [@Fisher1936], economists [@Haavelmo1943;@Rubin1974], geneticists [@Wright1934], and even computer scientists [@Pearl2009]. We can view causal inference as using theory and expert institutional knowledge to estimate the impact of events or decisions on a given outcome of interest [@Cunningham2021]. A naive assumption would be that prediction algorithms in ML cannot provide the rigour of empirical econometric design in extracting causal inference. But there is a growing sub-field of ML which tackles causality in two ways. Firstly, it can improve the predictive power of traditional econometrics by decoupling the search for relevant predictors from the search for specification [@Lopez_de_Prado2018].  Secondly, machine learning can play a key role in discovering new financial theories beyond the reach of traditional methods, such as a new theory in market microstructure that was used to predict the 2010 flash crash [@Easley2020].

### Hypotheses, assumptions and cultural clashes

Traditionally, machine learning is data-driven, while econometrics is hypothesis-driven, where valid inference from testing stands on model assumptions being the ground truth asymptotically. Over 20 years ago, the Berkeley statistician, Leo Breiman, lambasted the statistical community for their dogmatic approaches in the face of emerging algorithmic techniques to statistical science successes. He framed his argument as a culture problem where 

>..the statistical community has been committed almost exclusively to data models...where one assumes that a given stochastic data model generates the data. [@Breiman2001]

For the most part, the statistical community has now accepted machine learning (ML) as a standard part of statistical science, with graduate-level standards incorporating ML techniques alongside the traditional statistical approaches [@Hastie2009;@Efron2016] and leading statisticians exposing their benefits for enhancing scientific discovery [@Spiegelhalter2019].

While the statistics community has moved on, the economics and econometrics community has been much slower to depart from the strictness of data-generating models which embody consistency, normality and efficiency. The econometric canon pre-dates the dawn of digital computing, with models devised for estimation by hand. These are legacy technologies that need updating for the digitally savvy graduates of the future.  

ML approaches do not naturally deliver these theoretical properties^[Technically, the No Free lunch theorem applies has been applied to machine learning [@Wolpert1997]. This states that `a priori` no one learning algorithm can be defines as the *best* performer. Machine learning experts have argue that relevance of this criticism in recent years as research in statistical inference in machine learning develops [Giraud-Carrier, Christophe, and Foster Provost. “Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper.” In Proceedings of the ICML-2005 Workshop on Meta-learning, pp. 12–19. 2005.;Whitley, Darrell, and Jean Paul Watson. “Complexity theory and the no free lunch theorem.” In Search Methodologies, pp. 317–339. Springer, Boston, MA, 2005.]], but leading econometricians argue that if their discipline is to remain relevant for students, a balance must be struck between *using data to solve problems*^[This is framing econometrics as decision making under uncertainty[@Dreze1972;@Chamberlain2000;@Chamberlain2020]]
while preserving the strengths of applied econometrics [@Athey2019a].  Encouragingly, recent advances in theoretical properties of machine learning models published in econometrics[@Athey2017;@Wager2017;@Athey2019b;@Athey2019c] and applied statistics journals [@Zuo2021;@Apley2020].

The boundary between econometrics and ML is subject to much debate [@Lommers2021]. However, in applied work, the reality is much more nuanced, with many methods falling into both camps. For instance, the bootstrap facilitates statistical inference and ensemble methods, such as the Random Forest algorithm. 

Classical econometrics requires a model that incorporates our knowledge of the economic system^[The more popular frequentist paradigm depends on the behaviour of estimators under increasing sample size falls under the heading of “asymptotic theory.” The properties of most estimators in the classical world can only be assessed “asymptotically,” i.e. are only understood for the hypothetical case of an infinitely large sample. Also, virtually all specification tests used by frequentists hinge on asymptotic theory. This is a major limitation when the data size is finite[@Dixon2020].], and ML requires us to choose a predictive algorithm with reliable empirical capabilities. Justification for an inference model typically rests on whether we feel it adequately captures the essence of the system. Likewise, the choice of pattern-learning algorithms often depends on measures of past performance in similar scenarios. Thus, inference and ML can be complementary in pointing us to economically meaningful conclusions.

## Brief history of computing in finance and the cloud

For centuries, finance and computation have gone hand in hand, with quantitative finance taking its roots from Bachelier's *Theory of Speculation* [@Bachelier1900]. Computing as a utility can be traced back to Professor John McCarthy in the early 1960s.  As computing power has become more accessible and affordable, computation has become a central part of finance.  Figure 1 illustrates some of the critical moments in the development of computing in finance and the cloud.

```{r timeline, out.width="100%", fig.cap=" Computing landmarks finance and cloud computing.   The data for the cloud computing timeline is sourced from Varghese et al. (2019), while the finance timeline is the authors' calculations"}
library(ggrepel)
library(ggdark)
library(stringr)
library(tidyverse)

c("Harry Markowitz  introduces  Critical Line Algorithm",
  "NASDAQ launched  as first electronic  communications market",
  "Fischer Black  proposes idea of fully  electronic exchanges  in a landmark paper",
  "Black-Scholes-Merton  model for derivative pricing",
  "Jim Simons  Founded Renaissance Technologies (RenTec),  introducing complex mathematical trading algorithms to markets",
  "Michael Bloomberg  launches Innovative Market Systems  (which become Bloomberg LLP)",
  "RenTec's Medallion fund  launched,  later to become  the most successful hedge fund in history",
  "Heston & Dupire  introduce stochastic volatility models",
  "Jump diffusion  models introduced",
  "SECorder US stockexchanges to be decimalised",
  "Flash Crash  (Markets drop 10% in a matter of minutes)",
  "Basel III  requires to periodical estimate  counterparty risk of complex derivatives",
  "Quantum computing  is proposed by Renbart et. al  for derivative pricing",
  "RenTex's Medallion  Fund average 66% annual return  over last 30 years",
  "CME Smart Stream  launched offering real-time  cloud-based market data")->financeMs
  

c("Professor JohnMcCarthy (MIT)  suggested computing will be sold as a utility",
  "IBM  virtualised operating systems",
  "ARPANET  launched by US Advanced  Research Project Agency  connect 4 university computer systems",
  "100,000  computers on Internet",
  "World Wide Web  lanuched with 1 million computers on Net",
  "Cloud Computing  as a concept  introduced in Compaq report",
  "Amazon (AWS)  launched as first public cloud service",
  "Big Data  as a concept  introduced by the OpenNebula research project launched",
  "Elastic Computing (EC2)  launched by Amazon",
  "Dropbox  launch cloud storage",
  "Microsoft  launch Azure cloud computing",
  "DigitalOcean  Droplets launched",
  "Real-time  streaming data on AWS",
  "Machine learning  sold as a service in the Cloud",
  "Massive data-center  under the Altantic ocean  launched by Microsoft",
  "Google TPU's  (Tensor Processing Units)  avaliable on the cloud,  introducing tensor-based mathematical to public")->cloudMs            
content<-c(financeMs,cloudMs)            
start   = paste0(c("1952","1971","1972","1973","1982","1983","1988","1993","2000","2001","2010","2013","2017","2018","2019",
                   "1961","1967","1969","1988","1991","1996","2002","2005","2006","2007","2010","2012","2013","2015","2018","2019"),"-01-01")

for (i in seq_along(start)) {
  dy=sample(1:25,1)
  mth=sample(1:12,1)
  newStart=paste0(start[i],"-",mth,"-",dy)
  start[i]=newStart
}
i<-length(financeMs)
j=length(cloudMs)


timeline_data<-tibble(
  event = str_wrap(content,width=20),
  start=as.Date(start),
  group=c(rep("Finance",i),rep("Cloud computing",j))) %>%
  mutate(y_pos=ifelse(group=="Finance",0.001,1))
timeline_data %>%
  ggplot(aes(x=start,y=y_pos,fill=group,label=event)) + geom_label_repel(size=1.3,arrow =arrow(length=unit(0.02,'npc')),box.padding = 1,segment.alpha = 0.5) + scale_x_date(date_labels = "%Y",date_breaks = "4 years",sec.axis = dup_axis(name = "")) +
  labs(y="",x="",fill="") + scale_fill_brewer(type = "qual") +
  theme(legend.position = "bottom",
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        text = element_text(size = 8))

```

On the *buy-side*, in the early 1950a, Harry Markowitz transforms quantitative approaches to portfolio management. For example, Markowitz solved a complex mean-variance portfolio optimisation problem using algorithmic programming. Meanwhile, in the early 1960s [Ed Thorp](https://en.wikipedia.org/wiki/Edward_O._Thorp) and [John Simons](https://en.wikipedia.org/wiki/Jim_Simons_(mathematician)), using computer-aided statistical algorithms, showed how arbitrage opportunities, unseen by traditional hedge fund managers, could be exploited to beat the market *consistently*.  

On the *sell-side* a game-changing breakthrough in the 1970s was a model to price derivative products [@Black1973;@Merton1973] (BSM model), resulting in the explosive growth of options markets [@Cesa2017]. Subsequently, weaknesses in the BSM model fuelled growth in financial computing. Quantitative researchers, with the increased availability of computing power, used more realistic continuous-time pricing models to estimate complex partial differential equations [@Reisinger2018].

# Teaching environment for computing

Much like teaching statistics and data science, embedding computing in a financial analytics course has three interconnected teaching advantages:

1. Produce interesting output with data (and code) within the first ten minutes of the first class;
A have a knock-on effect of challenge students to infer meaning from data and statistics from day one;
2. Get students to think about computation as an integral part of the finance curriculum[@Kaplan2007; @Cetinkaya-Rundel2018])
3. Demystify the [folk theorem of statistical computing](https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/) where students think that changing the computing environment improves their output;

A standard solution is to use computing labs to facilitate computation exercises. However, one downside to this approach is that instructors usually do not have administrative access and therefore struggle to accomplish basic maintenance tasks, such as pre-loading module-specific content.  Furthermore, this usually leads to a familiar environment for all courses, rather than specialised setups for more advanced computational methods. Finally, the most significant downside is that using computing labs discourages active engagement of computation in all aspects of the module. 

Our approach has been to use a browser-based cloud computing solution to provide a frictionless student experience in lectures and workshop sessions. Using the sizeable academic discount, we use the RStudio Teams enterprise software packages and manage student access using a container farm of dockerised instances. The Workbench product of the Teams suite (formerly RStudio server pro) is the web server software that allows online access to several integrated development environments (IDEs)^[To date, the software ships with a Launcher package that facilitates access to Jupyter notebooks, Jupyterlab, RStudio IDE, and Visual Studio] to script in both R and Python [@RStudioW2021].

Compared to the computer labs approach, our approach has three distinct benefits:

The passive lecturing then active labs are replaced by dynamic lectures and labs and 24/7 access to computing for active independent learning;
Help students who have cost constraints or limitation to accessing computing hardware;
Ease of sharing code, data and environments.

## Why R and Python?
R and Python are the two leading languages used in the industry for data analysis. Thus, to best prepare students to be competitive and perform on the job market, we made the explicit decision to teach both languages at master level^[At present, MSc in Quantitative Finance uses both languages, and we hope to expand this to all finance programmes and the new Actuarial Science masters in the future].  Although some notable holdouts teach econometrics using commercial graphical user interfaces(GUI), these languages have infiltrated academia.  Proponents of GUI-based econometrics teaching argue that teaching statistical concepts is less intimidating to beginners when using a point-and-click approach than command line methods. Furthermore, the argument goes that teaching programming and statistics in tandem creates too much friction for students.

In our experience, such convenience is only possible by removing data analysis from the course content and providing students with tidy, rectangular data.  But for modern financial data analytics, this approach is a disservice to students.  Furthermore, point-and-click procedures require a bespoke student user manual that can run to [40-plus pages](https://github.com/barryquinn1/FMLmaterial/blob/27d8094fee39fa0284d3a0bfc10e38dcd3bebcac/Introducing%20Stata.pdf). 

We argue there is a significant learning curve for the novice student, which isn't generalisable to other analytics workflows.  In general, using a GUI *copy and paste* workflow can increase student frictions, be more error-prone, be harder to debug, and, most importantly, disconnect the logical link between computing from financial analytics[@Baumer2014]. But, perhaps most important is that by learning generalisable coding/data skills, a student an adequately prepared to into an industry where technologies are rapidly evolving. 

## Why RStudio Teams?

Figure 1 visualises the components that make up the RStudio Team bundle. 

```{r rstudioteams, fig.cap="The three components of the RStudio Enterprise Team Bundle",out.width="70%"}
knitr::include_graphics('img/Team.png',auto_pdf = T)
```


RStudio describes this product as follows: 

>RStudio Team is a bundle of RStudio's enterprise-grade professional software for scaling data science analytical work across your team, sharing data science results with your key stakeholders, and managing R and Python packages. RStudio Team includes RStudio Workbench, RStudio Package Manager, and RStudio Connect.
RStudio Team offers convenience, simplicity, and savings to organisations using R, Python and RStudio at scale. 

- [@RStudioT2021]

Teams is an enterprise-grade setup offered free of charge for academic teaching.  This discount is a significant saving for educational budgets, typically between \$15,000 to \$20,000.  The School's budget can then focus on purchasing an agile computing infrastructure.  

For teaching computation, the IDE is the most critical tool in this bundle. The Workbench product comes with Jupyter (notebook and lab) and RStudio native IDE, which provide a powerful interface that helps flatten the learning curve in command line teaching. It has a series of panes to view data, files, and plots interactively. Additionally, since it is a full-fledged IDE, it also features integrated help, syntax highlighting, and context-aware tab completion.

Students access the RStudio IDE through a centralised RStudio server instance, which allows us to provide students with uniform computing environments. Furthermore, the IDE integrates directly with some critically essential tools for teaching best practices and reproducible research, such as R Markdown, Docker, and Git version control. 

Importantly, we do not dissuade students from creating local instances of R and Python, but we do not want it to be a prerequisite of any module. Students are then allowed to progressively develop their setup to know that fully-fledged instances are always departmental resources.

## Remote RStudio Workbench Platform

A popular approach to running a centralised RStudio server in teaching computation in higher-level statistics courses is to build a shared infrastructure with high powered computation power.  This hardware is usually housed securely on-premises and managed by a dedicated IT team.  For example, the Duke University statistics department purchased and operated a powerful farm of computer servers that can serve approximately 100 students per semester [@Cetinkaya-Rundel2018].  We have chosen to run RStudio Workbench using virtualised hardware on the Microsoft Azure cloud.  Figure 3 shows the architecture of the current setup (without dockerisation).  Each student is assigned a Linux account, authenticated using a departmental login.  Students then connect to a single RStudio Workbench instance, and via the Launcher, the software can open an IDE to access Python or R scripting environments. Thus, each student experiences a similar computing environment solving the perennial.
[**but it worked on my machine?**](https://www.kevinwanke.com/why-you-should-never-use-the-phrase-but-it-works-on-my-machine/) problem.

The primary advantage of running and managing a cloud computing platform is control.  Lecturers control a shared user environment for each course, including required packages, resource configuration, remove or kill sessions and monitor resource demand on the system. This management work adds a considerable burden to the lecturer and the IT support, partially offset by the time saved supporting the build of lab-based PCs. However, our experience and student feedback suggest that the benefits far outweigh these additional costs.  Furthermore, not providing students with such a resource is a disservice to their employability in the modern world of finance.    

```{r current-setup, fig.cap="Current set up of RStudio workbench on Azure",out.width="70%"}
knitr::include_graphics("img/rstudiowb.png",auto_pdf = TRUE)
```


## Containerisation in finance

Linux containers are technologies that allow you to package and isolate applications with their entire runtime environment [@banker2017].  Their strategic advantage is their application independence from the underlying operating environment enabling standardisation and automation, significantly lowering cost and operational risk.  

Virtualisation technology is the underlying element of cloud computing, and containers take this to the next level. Cloud computing has traditionally used virtual machines to distribute available resources and provide isolated environments among users. The key difference between virtual machines and containers is that containers share the same underlying operating system [@Mavridis2019]

Containerisation is decades old, but the emergence of the open-source [Docker Engine](https://www.ibm.com/cloud/learn/docker) has accelerated the adoption of this technology. Docker is a *lightweight* virtualisation technology that allows sharing one operating system so that all code, runtimes, tools, and libraries needed for a piece of software are made available. This *build once run anywhere* property makes them highly portable, agile and efficient approach to running **sandboxed** instances of RStudio Workbench.  The open-source nature of Docker makes it a transparent and powerful tool for reproducible computational finance research. From a teaching perspective, each student can be mapped to a single container, secluding individual operates and maintaining strict control of computing resource usages to provide accidental disruption of individual student's work.

Furthermore, clusters can be deployed using a container orchestration system such as [Kubernetes](https://www.ibm.com/cloud/learn/kubernetes), and the operational overhead can be largely automated using [AKS](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes).  Given they are much lighter weight than VMs, a large container farm of RStudio instances can be run concurrently on one single server.  We plan to build this infrastructure into our platform and have sketched out the planned setup in figure 4.

```{r future-setup, fig.cap="Dockerised set up of RStudio workbench on Azure",out.width="80%"}
knitr::include_graphics("img/rstudiowb_kb.png",auto_pdf = TRUE)
```

One big challenge with Kubernetes is its steep learning curve, and even though Azure offers an automating management service, an administrator will still need to manage individual instance housekeeping.  For this reason, we opted for a more straightforward approach used in Duke University's statistical department and was created by Mark McCahill. He kindly shared his setup (https://gitlab.oit.duke.edu/mccahill/docker-rstudio), which we use to create a strict sandboxed virtual environment for each student.

# Course implementation

We piloted our new infrastructure at masters level teaching in the 2020-2021 academic year at Queen's Management School.  Named Q-RaP (Queen's management school Remote analytics Platform), students used the platform in two modules;  algorithmic trading and investment and time-series financial econometrics. Anecdotally, it received excellent feedback from students, especially when remote teaching and learning was the norm.  In 2021/2022, it will be used in a further two masters level courses (research methods in finance and computational methods in finance) and available for some business analytics modules. As well as the teaching advantages, the resource has the additional benefit of easing the demand pressures on computer labs.

## Reproducibility with computational notebooks

Computational notebooks are documents that combine code, discussion and output in a dynamic reproducible format. An essential advantage of computational notebooks is that they embody the PPDAC credible analysis workflow (Problem, Plan, Data, Analysis, Communication). PPDAC is the professional standard for data analysis and plausible inference[@Spiegelhalter2019]. Unlike the copy and paste approach, all five parts of the PPDAC approach can be included in one document, providing an enhanced level of transparency, portability and reproducibility.

There are two main formats for producing computational notebooks; Jupyter notebooks and R Markdown. Both are based on Markdown, one of the most popular markup languages in computing. Using Markdown is different from using a WYSIWYG editor. In an application like Microsoft Word, you click buttons to format words and phrases, and the changes are visible immediately. In contrast, when creating a Markdown-formatted file, you add Markdown syntax to the text to indicate which words and phrases should look different. Markdown is highly portable, platform-independent, future proof, and essential for the modern financial data scientist. 

Out of the box, the Jupyter ecosystem supports python scripting using the IPython kernel but can support up to 100 different languages (called 'kernels') by installing additional kernels^[https://jupyter4edu.github.io/jupyter-edu-book/jupyter.html]. Jupyter notebooks are a lightweight, low learning curve approach to teaching computing and are an excellent way to get non-technical students up and running in the first 10 minutes of a course. R Markdown is probably one of the most powerful tools in the RStudio IDE. R Markdown files are plain text documents that combine text, code and YAML metadata into an authoring framework for financial analytics. In the RStudio IDE, you can open an. Rmd file and working interactively, or render the file to build a static report or a dynamic web app using the `Shiny` packages.  For instance, when you render an R Markdown document, it will combine the text with output from your code.  The rendering process produces static formats such as HTML, pdf and word, but it can also produce interactive dashboards, web apps, slide shows, websites and more technical documentation (See video below). We mainly use Python and R code chunks in our teaching, the former output in the RStudio environment using the `reticulate` package.


Pedagogically, the main benefit of R Markdown and Jupyter notebooks is to embed the logical connection between computing and financial data analysis. This approach is sometimes referred to as *literate programming* [@Knuth1984]^[Donald Knuth is pioneering in the computing world and create the vastly popular TeX typesetting markup language], which made code, output and narrative inseparable.  Computational notebooks have four advantages over the copy-and-paste approach: 

1. Combining code and output in one document makes it easier for a student to locate the source of the errors and encourages more experimentation;
2. Strict uniformity of the reporting template makes it easier for the lecturers to grade;
3. Collaboration and group projects become much easier for students when using version control. Version control also provides a strict tagging system of individual contribution is assessed within a group work setting;
4. Provides a baseline template document that, as students learn, can be more and more lightweight. 
    * By removing the scaffolding in a slow, piecemeal way as the course progresses, active learning appeals.

On balance, using *literate programming* via computational notebooks has meaningful learning and employability benefits, especially as it is becoming a standard approach to collaboration in the finance industry. 

## Version control, git and GitHub

Increasingly, in the world of computational finance, version control is being used to disseminate and promote innovative coding solutions to financial problems. Furthermore, in line with applied statistics curricula [@Cetinkaya-Rundel2018], modern finance curricula should strive to have students produce reproducible output. Git is a popular command-line version control tool that integrates well with RStudio Teams. In addition, GitHub is a web-based hosting repository platform that provides access control and many more collaborative features to manage teamwork on computing projects.

From a finance industry employability perspective, in the past, there has been considerable resistance to the user of externally hosted IT services as security is paramount to highly regulated financial institutions.  The opposition has typically been for strategic and economic reasons:

* For companies that have swallowed the Windows *Koolaid* there are more secure options such as [Mercurial](https://www.mercurial-scm.org/)
* It is cheaper for large companies to do it in house
* In a large organisation, there are guaranteed to be fiefs all wanting to do things their way and a standardised version control system is the only appeal of there is an obvious [Total Cost of Ownership](https://www.investopedia.com/terms/t/totalcostofownership.asp) benefits.
These arguments are now outdated, especially with Big Tech acquisition activity in the git ecosystem space. For example, in [2018](https://www.bloomberg.com/news/articles/2018-06-03/microsoft-is-said-to-have-agreed-to-acquire-coding-site-github),  Microsoft bought GitHub and soon after [Alphabet's Google Ventures](https://www.bloomberg.com/news/articles/2018-09-19/alphabet-backs-gitlab-s-quest-to-surpass-microsoft-s-github) took a significant stake in GitLab. This has propelled git version control as an industry standard that is now easily integrated into all legacy systems, including Windows Servers.

Students are required to use git for all assignments in the classroom, where GitHub is a central repository where students can upload their work and provide feedback. Recently [GitHub Classroom](https://classroom.github.com/) was introduced, providing an enterprise-level service free of charge for academic teaching. 

Before GitHub classrooms, GitHub management tools such as organisation and teams can be set up privately so that only the students or the group of students can see and contribute to the assignment.  For example, we used a model where each module has a separate organisation to which students are invited at the beginning of the semester. For group work, the teams' tool allows creating a separate team-based repository with finer-grained access control. In addition, the instructor can monitor each student's progress and contribution with administrative access through the continuous integration functionality. GitHub classroom provides automated instant feedback on simple process tasks, for example, checking for common reproducibility mistakes in R Markdown submissions. Feedback on larger prediction projects can be automated using instant accuracy scores and live leader-boards similar to a Kaggle contest [@Cetinkaya-Rundel2018].  

Much of what has been described above has now been automated in GitHub Classroom and can also be integrated into learning management systems such as [Canvas](https://docs.github.com/en/education/manage-coursework-with-github-classroom/teach-with-github-classroom/connect-a-learning-management-system-to-github-classroom).  The learning curve for these tools is unavoidable. It can be high for introductory-level courses, but a basic understanding of the workflow in Figure 5 is sufficient for most modules.

```{r github-workflow, fig.cap="Seven git commands students need to learn"}
DiagrammeR::mermaid("
sequenceDiagram
  Working Directory->> Staging Area: git add
  Note left of Staging Area: `git add .` adds all changes
  Staging Area-->> Working Directory: git rm
  Note left of Staging Area: `git rm .` unstages all changes
  Staging Area->>Local Repository: git commit
  Local Repository->>Remote Repository: git push
  Note left of Remote Repository: pushed to GitHub
  Remote Repository->>Local Repository: git pull
  Local Repository->>Working Directory: git checkout
  Remote Repository->>Working Directory: git clone
")

```

# Discussion

As finance educators, our primary objective is to foster industry-ready graduates for the fast-paced digital age.  As we enter a new phase in the development cycle of financial technology, exposing students to industry-standard computing technologies is a good start.  Our goal with Q-RaP is to reduce the frictions of teaching computation in finance. Our vision is to expand this platform to all quantitative modules in the Management School.

Pedagogically, by embedding computation in a centralised frictionless way, we can spend more time developing the essential communications skills for explaining the *why* of the output from the code and data. Teaching econometrics and statistics in business schools is a considerable challenge, especially with students from non-technical backgrounds. The traditional approach off-the-shelf textbook exercises using mathematical formulas only serves to disenfranchise students from statistical computing further and is a disservice to the modern business school graduate. We find the learning curve is significantly flattened by a code-first approach, increasing student buy-in with approachability and usability. In addition, mathematical formulas can be introduced to build a deeper understanding of statistical plumbing and critical thinking around limitations.

The infrastructure and toolkit we described above ensure buy-in by making computing a central component of courses and assessments.  Using GitHub as the sole course management tool forces students to become familiar early, ensuring questions and problems are dealt with at least before the first assignment date.  Furthermore, requiring students to submit assignments using R Markdown forces students to using a literate programming approach, ensures reproducibility and embed the PPDAC principles in their work. Finally, from an employability perspective, indoctrinating students early with these reproducibility and workflow principles inoculates any bad computational habits forming, which are much harder to retrain out of financial researchers. 

Importantly, we want to enable students and colleagues to centralise computation in frictionless and agile education. We hope this can result in a more meaningful approach to *solving business problems with data* in a more thoughtful, transparent and significant matter.  But, perhaps most important is that by learning generalisable coding/data skills, a student an adequately prepared to into an industry where technologies are rapidly evolving. 
